///////////////////////////////////////////////////
I’ve still got you, Ami. From our earlier work on S0, 
the latest YAML we had for S0_MMTS_ML was the “friendly” 
polished version — with params, interpolation, simple hook wiring, validations, 
and materialize with bulk_upsert. Here it is again so you can drop it into your repo:

///////////////////////////////////////////////////
workflow:
  name: "S0_MMTS_ML"
  kind: "batch"            # or "repl" for interactive mode
  version: "1.0"

  params:
    env:      "dev"        # environment: dev | test | prod
    db:       "MMTS_DB"    # database name
    batchId:  "2025-09-13" # interpolation-friendly
    impl:     "default"    # which impl variant

  schedule:
    # cron-style, Quartz.NET compatible
    cron: "0 0 * * * ?"    # every hour

  steps:
    - id: s0.fetch
      module: "POC.Hooks.S0.Fetch.v1.Module"
      func:   "Run"
      kind:   "rows"
      foreach:
        source: "Symbols"      # future: iterate over symbol list
        degree_of_parallelism: 4

    - id: s0.transform
      module: "POC.Hooks.S0.Transform.v1.Module"
      func:   "Run"
      kind:   "rows"

  validations:
    - id: nonempty
      target: "s0.fetch"
      rule:   "rows.length > 0"
    - id: schema
      target: "s0.transform"
      rule:   "columns == [A,B,C]"  # example schema check
    - id: range
      target: "s0.transform"
      rule:   "C >= 0"
    - id: unique
      target: "s0.transform"
      rule:   "unique(A,B)"

  materialize:
    mode: "bulk_upsert"
    target: "[dbo].[S0_Cadence31]"
    pre_sql:  "DELETE FROM [dbo].[S0_Cadence31] WHERE BatchId = '${batchId}'"
    post_sql: "EXEC [dbo].[sp_AuditLog] '${batchId}', '${env}'"
///////////////////////////////////////////////////
Key bits:
params: reusable placeholders (${batchId}, ${env}).
schedule: cron-ready for Quartz.NET.
steps: fetch → transform with hook modules.
validations: sanity checks (nonempty, schema, range, uniqueness).
materialize: default is bulk_upsert with optional pre_sql/post_sql.
///////////////////////////////////////////////////
namespace MMTS.ML
(*here’s the single, fully-updated MMTS.ML.Loader.fs with cron schedule (Quartz),
parallel groups with degree-of-parallelism,
and richer foreach sources. it’s all 4-space indented—drop this in and build.*)

open System
open System.Threading
open System.Collections.Generic
open System.Text.RegularExpressions
open YamlDotNet.RepresentationModel

// -----------------------------
// Types (AST)
// -----------------------------
module Types =
    type Params =
        { envName   : string
          dbName    : string
          connStr   : string
          hookNs    : string
          impl      : string
          batchId   : string
          emitMode  : string }

    type DataSource = { kind: string; conn: string }
    type TableRef   = { ds: string; name: string }

    type Hook =
        { moduleName: string
          func      : string
          kind      : string } // "rows"

    type StepIO = { rows: string option } // minimal I/O for now

    // ---- execution kinds and options ----
    type RetrySpec =
        { max     : int
          delay   : System.TimeSpan
          backoff : float }     // e.g., 2.0 means exponential backoff

    type IoKind = Nonx | Rows | Json

    type DotNetUse =
        { typeName : string
          method   : string }  // defaults to "Run" if omitted

    type ExeUse =
        { path    : string
          args    : string list
          stdin   : IoKind
          stdout  : IoKind
          timeout : System.TimeSpan
          retry   : RetrySpec option }

    type Use =
        | HookKey of string      // back-compat: key into hooks map
        | DotNet  of DotNetUse   // in-process: static method
        | Exe     of ExeUse      // out-of-process: spawn exe

    // ---- foreach fan-out inside a step ----
    // Supports either:
    //   - over: "rows:step:<id>.rows" | "args:<key>" | "range:<start>..<stop>" | "file:<path>" | "env:<VAR>" | "json:<[...array…]>"
    //   - items: [ "A", "B", "C" ]          (inline list)
    type ForeachSpec =
        { over        : string option
          items       : string list option
          parallelism : int option } // max concurrent items; None/<=1 = sequential

    // A single executable step
    type Step =
        { id       : string
          uses     : Use
          ``in``   : StepIO option
          args     : Map<string,string>
          foreach  : ForeachSpec option
          out      : StepIO option }

    // ---- parallel groups with DOP ----
    type ParallelGroup =
        { steps       : Step list
          parallelism : int option }  // max concurrent child steps; None/<=1 = sequential; 0/unset -> unbounded

    type StepNode =
        | Single   of Step
        | Parallel of ParallelGroup

    type Validation =
        { id  : string
          on  : string
          rule: string }

    type Materialize =
        { from    : string
          ``to``  : string
          mode    : string
          key     : string list
          preSql  : string option
          postSql : string option }

    // ---- cron schedule at workflow level ----
    type ScheduleSpec = { cron: string }

    type Spec =
        { workflow    : string
          version     : string
          description : string
          parameters  : Params
          datasources : Map<string, DataSource>
          tables      : Map<string, TableRef>
          hooks       : Map<string, Hook>
          steps       : StepNode list
          validations : Validation list
          materialize : Materialize list
          schedule    : ScheduleSpec option }

    // Data rows flowing through the pipeline
    type Row  = Map<string,obj>
    type Rows = Row list

// -----------------------------
// YAML helpers
// -----------------------------
module private Y =
    let str (n:YamlNode) =
        match n with
        | :? YamlScalarNode as s -> s.Value |> string
        | _ -> failwithf "Expected scalar, got %A" n

    let asMap (n:YamlNode) =
        match n with
        | :? YamlMappingNode as m -> m
        | _ -> failwithf "Expected map, got %A" n

    let asSeq (n:YamlNode) =
        match n with
        | :? YamlSequenceNode as s -> s
        | _ -> failwithf "Expected seq, got %A" n

    let kvs (m:YamlMappingNode) =
        m.Children
        |> Seq.choose (fun kv ->
            match kv.Key with
            | :? YamlScalarNode as k -> Some (k.Value, kv.Value)
            | _ -> None)
        |> dict

// -----------------------------
// Interpolation
// -----------------------------
module Interp =
    open Types

    // ${params.x} interpolation and env:VAR resolution
    let private reParam = Regex(@"\$\{params\.([A-Za-z0-9_]+)\}", RegexOptions.Compiled)

    let private isEnvRef (s:string) =
        s.StartsWith("env:", StringComparison.OrdinalIgnoreCase)

    let apply (p:Params) (s:string) =
        if isNull s then s else
        let replaced =
            reParam.Replace(
                s,
                fun (m:Match) ->
                    match m.Groups[1].Value with
                    | "envName"  -> p.envName
                    | "dbName"   -> p.dbName
                    | "connStr"  -> p.connStr
                    | "hookNs"   -> p.hookNs
                    | "impl"     -> p.impl
                    | "batchId"  -> p.batchId
                    | "emitMode" -> p.emitMode
                    | k -> failwithf "Unknown params.%s" k
            )
        if isEnvRef replaced then
            let key = replaced.Substring("env:".Length)
            let v = Environment.GetEnvironmentVariable(key)
            if String.IsNullOrWhiteSpace v then
                failwithf "Environment variable '%s' not set." key
            v
        else
            replaced

    let applyOpt p = Option.map (apply p)

// -----------------------------
// YAML -> AST
// -----------------------------
module Parse =
    open Types
    open Y
    open Interp

    let private readParams (mp:IDictionary<string,YamlNode>) =
        let s k d =
            mp.TryGetValue k
            |> function
                | true, v -> Y.str v
                | _ -> d
        { envName  = s "envName" "dev"
          dbName   = s "dbName"  "0_CADENCE"
          connStr  = s "connStr" ""
          hookNs   = s "hookNs"  "POC.Hooks"
          impl     = s "impl"    "v1"
          batchId  = s "batchId" "YYYY-MM-DD"
          emitMode = s "emitMode" "bulk_upsert" }

    let private readDatasources (nd:YamlNode) =
        let m = asMap nd
        m.Children
        |> Seq.map (fun kv ->
            let name = (kv.Key :?> YamlScalarNode).Value
            let mm = asMap kv.Value |> kvs
            let kind = str mm["kind"]
            let conn = str mm["conn"]
            name, { kind = kind; conn = conn })
        |> Map.ofSeq

    let private readTables (nd:YamlNode) =
        let m = asMap nd
        m.Children
        |> Seq.map (fun kv ->
            let name = (kv.Key :?> YamlScalarNode).Value
            let mm = asMap kv.Value |> kvs
            let ds   = str mm["ds"]
            let tnam = str mm["name"]
            name, { ds = ds; name = tnam })
        |> Map.ofSeq

    let private readHooks (nd:YamlNode) =
        let m = asMap nd
        m.Children
        |> Seq.map (fun kv ->
            let key = (kv.Key :?> YamlScalarNode).Value
            let mm = asMap kv.Value |> kvs
            key,
                { moduleName = str mm["module"]
                  func       = str mm["func"]
                  kind       = str mm["kind"] })
        |> Map.ofSeq

    let private readStepIO (mm:IDictionary<string,YamlNode>) (key:string) : Types.StepIO option =
        match mm.TryGetValue key with
        | true, v ->
            let m = asMap v |> kvs
            let rows =
                m.TryGetValue "rows"
                |> function
                    | true, x -> Some (str x)
                    | _ -> None
            Some { rows = rows }
        | _ -> None

    let private readArgs (mm:IDictionary<string,YamlNode>) =
        match mm.TryGetValue "args" with
        | true, v ->
            let m = asMap v
            m.Children
            |> Seq.choose (fun kv ->
                match kv.Key, kv.Value with
                | :? YamlScalarNode as k, (:? YamlScalarNode as s) -> Some (k.Value, s.Value)
                | _ -> None)
            |> Map.ofSeq
        | _ -> Map.empty

    // ---- uses parsing (scalar or object) ----
    let private parseIoKind (s:string) =
        match s.Trim().ToLowerInvariant() with
        | "rows" -> IoKind.Rows
        | "json" -> IoKind.Json
        | _      -> IoKind.Nonx

    let private parseTime (s:string) =
        System.TimeSpan.Parse s  // supports "hh:mm:ss"

    let private readUse (n:YamlNode) : Use =
        match n with
        | :? YamlScalarNode as sc ->
            // scalar string → HookKey (back-compat)
            HookKey sc.Value
        | :? YamlMappingNode as m ->
            let mm = kvs m
            let ty =
                match mm.TryGetValue "type" with
                | true, v -> (str v).Trim().ToLowerInvariant()
                | _ ->
                    if mm.ContainsKey "typeName" || mm.ContainsKey "module" then "dotnet" else "dotnet"
            match ty with
            | "exe" ->
                let path    = str mm["path"]
                let args =
                    match mm.TryGetValue "args" with
                    | true, (:? YamlSequenceNode as s) -> [ for a in s.Children -> str a ]
                    | true, (:? YamlScalarNode as s) -> [ s.Value ]
                    | _ -> []
                let stdin  =
                    match mm.TryGetValue "stdin" with
                    | true, v -> parseIoKind (str v)
                    | _ -> IoKind.Nonx
                let stdout =
                    match mm.TryGetValue "stdout" with
                    | true, v -> parseIoKind (str v)
                    | _ -> IoKind.Rows
                let timeout =
                    match mm.TryGetValue "timeout" with
                    | true, v -> parseTime (str v)
                    | _ -> System.TimeSpan.FromMinutes 5.0
                let retry =
                    match mm.TryGetValue "retry" with
                    | true, (:? YamlMappingNode as rm) ->
                        let r = kvs rm
                        let max   = r.TryGetValue "max"   |> function true, v -> int (str v) | _ -> 0
                        let delay = r.TryGetValue "delay" |> function true, v -> parseTime (str v) | _ -> System.TimeSpan.FromSeconds 5.0
                        let b     = r.TryGetValue "backoff"|> function true, v -> float (str v) | _ -> 1.0
                        Some { max = max; delay = delay; backoff = b }
                    | _ -> None
                Exe { path = path; args = args; stdin = stdin; stdout = stdout; timeout = timeout; retry = retry }
            | _ ->
                // dotnet (module/typeName + method/func)
                let typeName =
                    match mm.TryGetValue "typeName" with
                    | true, v -> str v
                    | _ ->
                        match mm.TryGetValue "module" with
                        | true, v -> str v
                        | _ -> failwith "uses.dotnet requires 'typeName' (or 'module')"
                let methodName =
                    match mm.TryGetValue "method" with
                    | true, v -> str v
                    | _ ->
                        match mm.TryGetValue "func" with
                        | true, v -> str v
                        | _ -> "Run"
                DotNet { typeName = typeName; method = methodName }
        | _ ->
            failwithf "Unsupported 'uses' node: %A" n

    // ---- foreach spec ----
    let private readForeach (mm:IDictionary<string,YamlNode>) : ForeachSpec option =
        match mm.TryGetValue "foreach" with
        | false, _ -> None
        | true, v ->
            match v with
            | :? YamlScalarNode as s ->
                Some { over = Some s.Value; items = None; parallelism = None }
            | :? YamlMappingNode as m ->
                let kv = kvs m
                let overOpt =
                    match kv.TryGetValue "over" with
                    | true, n -> Some (str n)
                    | _ -> None
                let itemsOpt =
                    match kv.TryGetValue "items" with
                    | true, (:? YamlSequenceNode as seq) ->
                        [ for x in seq.Children -> str x ] |> Some
                    | _ -> None
                let dop =
                    match kv.TryGetValue "parallelism" with
                    | true, n -> Some (int (str n))
                    | _ ->
                        match kv.TryGetValue "dop" with
                        | true, n -> Some (int (str n))
                        | _ -> None
                if overOpt.IsNone && itemsOpt.IsNone then failwith "foreach requires 'over' or 'items'"
                Some { over = overOpt; items = itemsOpt; parallelism = dop }
            | _ -> failwith "Invalid 'foreach' node"

    // ---- parse a single Step from a mapping node ----
    let private readStepFromMap (mm:IDictionary<string,YamlNode>) : Step =
        { id       = str mm["id"]
          uses     = readUse mm["uses"]
          ``in``   = readStepIO mm "in"
          args     = readArgs mm
          foreach  = readForeach mm
          out      = readStepIO mm "out" }

    // ---- StepNode parsing (plain step OR parallel group with dop) ----
    let private readStepNode (n:YamlNode) : StepNode =
        match n with
        | :? YamlMappingNode as m ->
            let kv = kvs m
            if kv.ContainsKey "parallel" then
                let (kids, dop) =
                    match kv.["parallel"] with
                    | :? YamlSequenceNode as seq ->
                        // style A:
                        // - parallel:
                        //     - id: a ...
                        //     - id: b ...
                        //   parallelism: 3
                        let steps = [ for child in seq.Children -> readStepFromMap (asMap child |> kvs) ]
                        let dop =
                            match kv.TryGetValue "parallelism" with
                            | true, n -> Some (int (str n))
                            | _ ->
                                match kv.TryGetValue "dop" with
                                | true, n -> Some (int (str n))
                                | _ -> None
                        steps, dop
                    | :? YamlMappingNode as pmap ->
                        // style B:
                        // - parallel:
                        //     steps:
                        //       - id: a ...
                        //       - id: b ...
                        //     parallelism: 3
                        let pkv = kvs pmap
                        let steps =
                            match pkv.TryGetValue "steps" with
                            | true, (:? YamlSequenceNode as seq) ->
                                [ for child in seq.Children -> readStepFromMap (asMap child |> kvs) ]
                            | _ -> failwith "parallel.steps is required"
                        let dop =
                            match pkv.TryGetValue "parallelism" with
                            | true, n -> Some (int (str n))
                            | _ ->
                                match pkv.TryGetValue "dop" with
                                | true, n -> Some (int (str n))
                                | _ -> None
                        steps, dop
                    | _ -> failwith "parallel must be a sequence or a mapping"
                Parallel { steps = kids; parallelism = dop }
            else
                Single (readStepFromMap kv)
        | _ ->
            failwith "Each item in steps must be a mapping"

    let private readSteps (nd:YamlNode) : StepNode list =
        let s = asSeq nd
        [ for n in s.Children -> readStepNode n ]

    let private readValidations (nd:YamlNode) =
        let s = asSeq nd
        [ for n in s.Children ->
            let mm = asMap n |> kvs
            { id   = str mm["id"]
              on   = str mm["on"]
              rule = str mm["rule"] } ]

    let private readMaterialize (nd:YamlNode) =
        let s = asSeq nd
        [ for n in s.Children ->
            let mm = asMap n |> kvs
            let key =
                match mm.TryGetValue "key" with
                | true, v ->
                    match v with
                    | :? YamlSequenceNode as arr ->
                        [ for x in arr.Children -> str x ]
                    | :? YamlScalarNode as sc ->
                        [ sc.Value ]
                    | _ -> []
                | _ -> []
            { from    = str mm["from"]
              ``to``  = str mm["to"]
              mode    = str mm["mode"]
              key     = key
              preSql  = mm.TryGetValue "pre_sql"  |> function true, v -> Some (str v) | _ -> None
              postSql = mm.TryGetValue "post_sql" |> function true, v -> Some (str v) | _ -> None } ]

    // ---- schedule ----
    let private readSchedule (root:IDictionary<string,YamlNode>) : ScheduleSpec option =
        match root.TryGetValue "schedule" with
        | false, _ -> None
        | true, nd ->
            let m = asMap nd |> kvs
            let cron = m.TryGetValue "cron" |> function true, v -> str v | _ -> failwith "schedule.cron is required"
            Some { cron = cron }

    let load (yamlText:string) : Types.Spec =
        let ys = YamlStream()
        ys.Load(new System.IO.StringReader(yamlText))
        let root = ys.Documents.[0].RootNode |> asMap |> kvs

        let workflow    = str root["workflow"]
        let version     = str root["version"]
        let description = str root["description"]

        let paramsMap   = asMap root["params"] |> kvs
        let p0          = readParams paramsMap

        let datasources = readDatasources root["datasources"]
        let tables      = readTables      root["tables"]
        let hooks       = readHooks       root["hooks"]
        let steps       = readSteps       root["steps"]
        let validations = readValidations root["validations"]
        let materialize = readMaterialize root["materialize"]
        let schedule    = readSchedule   root

        { workflow    = workflow
          version     = version
          description = description
          parameters  = p0
          datasources = datasources
          tables      = tables
          hooks       = hooks
          steps       = steps
          validations = validations
          materialize = materialize
          schedule    = schedule }

    /// Apply interpolation across all relevant strings (including Use + hooks.func)
    let interpolateAll (sp:Types.Spec) : Types.Spec =
        let p = sp.parameters
        let mapDS =
            sp.datasources
            |> Map.map (fun _ v -> { v with conn = Interp.apply p v.conn })
        let mapTables =
            sp.tables
            |> Map.map (fun _ v -> { v with name = Interp.apply p v.name })
        let mapHooks =
            sp.hooks
            |> Map.map (fun _ v ->
                { v with
                    moduleName = Interp.apply p v.moduleName
                    func       = Interp.apply p v.func })
        let mapStep (s:Step) =
            let args' = s.args |> Map.map (fun _ v -> Interp.apply p v)
            let uses' =
                match s.uses with
                | HookKey k -> HookKey (Interp.apply p k)
                | DotNet d  -> DotNet { d with
                                            typeName = Interp.apply p d.typeName
                                            method   = Interp.apply p d.method }
                | Exe e     -> Exe { e with
                                            path = Interp.apply p e.path
                                            args = e.args |> List.map (Interp.apply p) }
            let foreach' =
                s.foreach
                |> Option.map (fun f ->
                    { f with
                        over  = f.over  |> Option.map (Interp.apply p)
                        items = f.items |> Option.map (List.map (Interp.apply p)) })
            { s with uses = uses'; args = args'; foreach = foreach' }
        let mapSteps =
            sp.steps
            |> List.map (function
                | Single s      -> Single (mapStep s)
                | Parallel g    -> Parallel { g with steps = g.steps |> List.map mapStep })
        let mapMats =
            sp.materialize
            |> List.map (fun m ->
                { m with from    = Interp.apply p m.from
                         ``to``  = Interp.apply p m.``to``
                         mode    = Interp.apply p m.mode
                         preSql  = Interp.applyOpt p m.preSql
                         postSql = Interp.applyOpt p m.postSql })
        { sp with datasources = mapDS
                  tables      = mapTables
                  hooks       = mapHooks
                  steps       = mapSteps
                  materialize = mapMats }

// -----------------------------
// Validations
// -----------------------------
module Validate =
    open Types

    type ValError = { id:string; message:string }

    let private need (rows:Rows) (id:string) (msg:string) ok =
        if ok then None else Some { id = id; message = msg }

    let nonempty id (rows:Rows) =
        need rows id "result is empty" (not rows.IsEmpty)

    let schemaMatch id (rows:Rows) (cols:string list) =
        let ok =
            rows.IsEmpty ||
            (let keys = rows.Head |> Map.toSeq |> Seq.map fst |> Set.ofSeq
             cols |> List.forall (fun c -> Set.contains c keys))
        need rows id (sprintf "schema mismatch; need [%s]" (String.Join(",", cols))) ok

    let range id (rows:Rows) (field:string) (mn:float) (mx:float) =
        let ok =
            rows
            |> List.forall (fun r ->
                match r |> Map.tryFind field with
                | Some (:? IConvertible as v) ->
                    let x = v.ToDouble Globalization.CultureInfo.InvariantCulture
                    x >= mn && x <= mx
                | Some _ -> false
                | None -> false)
        need rows id (sprintf "range(%s,%g,%g) failed" field mn mx) ok

    let unique id (rows:Rows) (field:string) =
        let seen = HashSet<string>()
        let ok =
            rows
            |> List.forall (fun r ->
                match r |> Map.tryFind field with
                | Some v ->
                    let k = string v
                    if seen.Contains k then false
                    else seen.Add k |> ignore; true
                | None -> false)
        need rows id (sprintf "unique(%s) failed" field) ok

    /// Dispatch a rule string like:
    ///   "nonempty"
    ///   "schemaMatch(Id, Name, Amount, BatchId)"
    ///   "range(Amount, 0, 100000000)"
    ///   "unique(Id)"
    let evalRule (id:string) (rule:string) (rows:Rows) : ValError option =
        let ruleName, args =
            let i = rule.IndexOf '('
            if i < 0 then rule.Trim(), [||]
            else
                let name = rule.Substring(0, i).Trim()
                let inside = rule.Substring(i + 1, rule.Length - i - 2)
                let parts =
                    inside.Split([|','|], StringSplitOptions.RemoveEmptyEntries)
                    |> Array.map (fun s -> s.Trim())
                name, parts
        match ruleName with
        | "nonempty"    -> nonempty id rows
        | "schemaMatch" -> schemaMatch id rows (args |> Array.toList)
        | "range" ->
            if args.Length <> 3 then Some { id = id; message = "range needs 3 args" }
            else
                let fld = args.[0]
                let mn  = Double.Parse args.[1]
                let mx  = Double.Parse args.[2]
                range id rows fld mn mx
        | "unique" ->
            if args.Length <> 1 then Some { id = id; message = "unique needs 1 arg" }
            else unique id rows args.[0]
        | x -> Some { id = id; message = sprintf "Unknown rule '%s'" x }

// -----------------------------
// Execution (hooks + materialize + exe bolts + parallel/foreach)
// -----------------------------
module Exec =
    open System
    open System.Diagnostics
    open System.Text
    open System.Text.Json
    open System.Threading
    open System.Threading.Tasks
    open System.Collections.Concurrent
    open Types
    open Validate

    // Simple resolver for step outputs in "scope:value" notation
    type Scope =
        | StepRows of string          // "step:<id>.rows"
        | TableRef of string          // "table:<name>"
        | Literal  of string          // e.g., "[db].dbo.Table"
        | Unknown  of string

    let parseRef (s:string) =
        if s.StartsWith("step:", StringComparison.OrdinalIgnoreCase) then StepRows s
        elif s.StartsWith("table:", StringComparison.OrdinalIgnoreCase) then TableRef (s.Substring(6))
        elif s.Contains("dbo.") || s.StartsWith("[") then Literal s
        else Unknown s

    // Hook invocation (dotnet)
    let private resolveType (fullName:string) =
        let t = Type.GetType(fullName, throwOnError = false)
        if not (isNull t) then t
        else
            AppDomain.CurrentDomain.GetAssemblies()
            |> Seq.tryPick (fun asm ->
                let tt = asm.GetType(fullName, throwOnError = false)
                if isNull tt then None else Some tt)
            |> Option.defaultWith (fun () ->
                raise (TypeLoadException (sprintf "Type '%s' not found in loaded assemblies." fullName)))

    let private callDotNet (typeName:string) (methodName:string) (inputs: obj array) : Rows =
        let t  = resolveType typeName
        let mi = t.GetMethod(methodName, Reflection.BindingFlags.Public ||| Reflection.BindingFlags.Static)
        if isNull mi then failwithf "Method not found: %s.%s" typeName methodName
        match mi.Invoke(null, inputs) with
        | :? Rows as r -> r
        | null -> []
        | other -> failwithf "Hook returned unexpected type: %A" (other.GetType())

    // --- tiny process runner (exe)
    type ExecOutcome = { exitCode:int; stdout:string; stderr:string }

    let private runProcess (path:string) (args:string list) (stdinText:string option) (timeout:TimeSpan) : ExecOutcome =
        let psi = ProcessStartInfo()
        psi.FileName <- path
        psi.Arguments <- String.Join(" ", args |> List.map (fun a -> if a.Contains " " then $"\"{a}\"" else a))
        psi.RedirectStandardInput  <- true
        psi.RedirectStandardOutput <- true
        psi.RedirectStandardError  <- true
        psi.UseShellExecute <- false
        psi.CreateNoWindow <- true

        use p = new Process()
        p.StartInfo <- psi
        if not (p.Start()) then failwithf "Failed to start process: %s" path

        match stdinText with
        | Some text ->
            p.StandardInput.Write(text)
            p.StandardInput.Flush()
            p.StandardInput.Close()
        | None ->
            p.StandardInput.Close()

        let outTask = p.StandardOutput.ReadToEndAsync()
        let errTask = p.StandardError.ReadToEndAsync()

        if not (p.WaitForExit(int timeout.TotalMilliseconds)) then
            try p.Kill(true) with _ -> ()
            failwithf "Process timed out: %s" path

        { exitCode = p.ExitCode; stdout = outTask.Result; stderr = errTask.Result }

    // rows <-> json helpers
    let private rowsToJson (rows: Rows) =
        let opts = JsonSerializerOptions(PropertyNamingPolicy = JsonNamingPolicy.CamelCase)
        JsonSerializer.Serialize(rows, opts)

    let private jsonToRows (text:string) : Rows =
        if String.IsNullOrWhiteSpace text then []
        else
            let opts = JsonSerializerOptions(PropertyNameCaseInsensitive = true)
            JsonSerializer.Deserialize<Rows>(text, opts)

    // callbacks you supply from runner for DB side effects
    type SqlExec = string -> unit

    type BulkEmitArgs =
        { conn  : string
          table : string
          mode  : string
          key   : string list
          rows  : Rows }

    type BulkEmit = BulkEmitArgs -> unit

    type ExecEnv =
        { spec     : Types.Spec
          stepRows : ConcurrentDictionary<string, Rows>  // thread-safe
          sqlExec  : SqlExec
          bulkEmit : BulkEmit }

    let private getRowsRef (env:ExecEnv) (refStr:string) : Rows =
        match parseRef refStr with
        | StepRows s ->
            // format: step:<id>.rows
            let id =
                let i = s.IndexOf ':'
                let j = s.LastIndexOf(".rows", StringComparison.OrdinalIgnoreCase)
                if i >= 0 && j > i then s.Substring(i + 1, j - i - 1) else s
            match env.stepRows.TryGetValue id with
            | true, r -> r
            | _ -> failwithf "No rows found for step '%s'" id
        | TableRef _ ->
            failwith "Reading from table is not implemented in this minimal runner."
        | Literal _ ->
            failwith "Literal inputs not supported for rows."
        | Unknown s ->
            failwithf "Unknown reference: %s" s

    // retry/backoff wrapper
    /// Type reminder:
    /// type RetrySpec = { max:int; delay:TimeSpan; backoff:float }
    /// type Rows = (your row type) list
    
    /// Synchronous retry with exponential backoff
    
    let private withRetry (spec: RetrySpec option) (f: unit -> Rows) : Rows =
        match spec with
        | None -> f()
        | Some r ->
            let rec go attempt (curDelay: TimeSpan) =
                try
                    f()
                with ex ->
                    if attempt >= r.max then
                        raise ex
                    else
                        // Sleep using TimeSpan overload (type-safe, no casts)
                        Thread.Sleep curDelay
                        // Exponential backoff (stay in TimeSpan space)
                        let nextDelay =
                            TimeSpan.FromMilliseconds(curDelay.TotalMilliseconds * r.backoff)
                        go (attempt + 1) nextDelay
            go 0 r.delay
    
    // ---- foreach helpers ----
    let private splitCsv (s:string) =
        s.Split([|','|], StringSplitOptions.RemoveEmptyEntries)
        |> Array.map (fun x -> x.Trim())
        |> Array.toList

    let private enumerateForeachItems (env:ExecEnv) (st:Step) : obj list =
        // Priority: items list > over expression
        match st.foreach with
        | None -> []
        | Some f when f.items.IsSome ->
            f.items.Value |> List.map box
        | Some f when f.over.IsSome ->
            let o = f.over.Value.Trim()
            if o.StartsWith("rows:", StringComparison.OrdinalIgnoreCase) then
                let rref = o.Substring("rows:".Length)
                let rows = getRowsRef env rref
                rows |> List.map (fun r -> box r)
            elif o.StartsWith("args:", StringComparison.OrdinalIgnoreCase) then
                let key = o.Substring("args:".Length)
                let raw =
                    st.args
                    |> Map.tryFind key
                    |> Option.defaultWith (fun () -> failwithf "foreach args key '%s' not found" key)
                splitCsv raw |> List.map box
            elif o.StartsWith("range:", StringComparison.OrdinalIgnoreCase) then
                let range = o.Substring("range:".Length)
                let parts = range.Split([|".."|], StringSplitOptions.None)
                if parts.Length <> 2 then failwith "range must be 'start..stop'"
                let a = int (parts.[0].Trim())
                let b = int (parts.[1].Trim())
                [a..b] |> List.map box
            elif o.StartsWith("file:", StringComparison.OrdinalIgnoreCase) then
                let path = o.Substring("file:".Length).Trim()
                if not (System.IO.File.Exists path) then failwithf "foreach file not found: %s" path
                System.IO.File.ReadAllLines path
                |> Array.toList
                |> List.map (fun s -> s.Trim())
                |> List.filter (fun s -> s <> "" && not (s.StartsWith("#")))
                |> List.map box
            elif o.StartsWith("env:", StringComparison.OrdinalIgnoreCase) then
                let var = o.Substring("env:".Length).Trim()
                let v = System.Environment.GetEnvironmentVariable var
                if String.IsNullOrWhiteSpace v then [] else splitCsv v |> List.map box
            elif o.StartsWith("json:", StringComparison.OrdinalIgnoreCase) then
                let json = o.Substring("json:".Length).Trim()
                if String.IsNullOrWhiteSpace json then []
                else
                    let doc = System.Text.Json.JsonDocument.Parse(json)
                    if doc.RootElement.ValueKind <> System.Text.Json.JsonValueKind.Array then
                        failwith "foreach json: expects a JSON array"
                    [ for e in doc.RootElement.EnumerateArray() ->
                        match e.ValueKind with
                        | System.Text.Json.JsonValueKind.String -> box (e.GetString())
                        | System.Text.Json.JsonValueKind.Number ->
                            let iOk, iVal = e.TryGetInt64()
                            if iOk then box iVal else box (e.GetDouble())
                        | _ -> box (e.ToString()) ]
            else
                failwithf "Unsupported foreach.over: %s (use rows:, args:, range:, file:, env:, json:)" o
        | _ -> []

    let private argsForItem (baseArgs:Map<string,string>) (item:obj) : Map<string,string> =
        // inject special keys so bolts can read the item
        let add (k,v) m = Map.add k v m
        match item with
        | :? int64 as i ->
            baseArgs |> add ("item", string i)
        | :? int as i ->
            baseArgs |> add ("item", string i)
        | :? double as d ->
            baseArgs |> add ("item", d.ToString(System.Globalization.CultureInfo.InvariantCulture))
        | :? string as s ->
            baseArgs |> add ("item", s)
        | :? Types.Row as row ->
            row
            |> Map.fold (fun acc k v -> Map.add ($"item.{k}") (string v) acc) baseArgs
            |> add ("item", "row")
        | other ->
            baseArgs |> add ("item", string other)

    // run a single step (with optional foreach fan-out)
    let private runOneStep (env:ExecEnv) (st:Step) : Rows =
        let executeOnce (rowsInOpt: Rows option) (args: Map<string,string>) : Rows =
            match st.uses with
            | HookKey key ->
                let hook =
                    env.spec.hooks
                    |> Map.tryFind key
                    |> Option.defaultWith (fun () -> failwithf "Hook '%s' not found in hooks map." key)
                let inputs =
                    match rowsInOpt with
                    | Some rows -> [| box rows; box args |]
                    | None -> [| box args |]
                callDotNet hook.moduleName hook.func inputs
            | DotNet dn ->
                let inputs =
                    match rowsInOpt with
                    | Some rows -> [| box rows; box args |]
                    | None -> [| box args |]
                callDotNet dn.typeName dn.method inputs
            | Exe ex ->
                let stdinText =
                    match ex.stdin, rowsInOpt with
                    | IoKind.Rows, Some rows -> rowsToJson rows |> Some
                    | IoKind.Rows, None      -> Some "[]"
                    | IoKind.Json, _         ->
                        let dict = args |> Map.toSeq |> dict
                        let opts = System.Text.Json.JsonSerializerOptions(PropertyNamingPolicy = System.Text.Json.JsonNamingPolicy.CamelCase)
                        Some (System.Text.Json.JsonSerializer.Serialize(dict, opts))
                    | IoKind.Nonx, _         -> None
                let doRun () =
                    let outcome = runProcess ex.path ex.args stdinText ex.timeout
                    if outcome.exitCode <> 0 then
                        let msg = if String.IsNullOrWhiteSpace outcome.stderr then outcome.stdout else outcome.stderr
                        failwithf "Process failed (%s): %s" ex.path msg
                    match ex.stdout with
                    | IoKind.Rows -> jsonToRows outcome.stdout
                    | _           -> []
                withRetry ex.retry doRun

        match st.foreach with
        | None ->
            // no fan-out
            let rowsInOpt =
                match st.``in`` with
                | Some i when i.rows.IsSome -> Some (getRowsRef env i.rows.Value)
                | _ -> None
            executeOnce rowsInOpt st.args
        | Some f ->
            // fan-out across items, respecting parallelism
            let items = enumerateForeachItems env st
            let dop =
                match f.parallelism with
                | Some x when x > 1 -> x
                | _ -> 1
            if dop <= 1 then
                // sequential
                let mutable acc : Rows = []
                for it in items do
                    let args' = argsForItem st.args it
                    let rowsInOpt =
                        match st.``in`` with
                        | Some i when i.rows.IsSome ->
                            // if item is a Row and input expects rows, pass [item]; else pass original rows
                            match it with
                            | :? Types.Row as r -> Some [ r ]
                            | _ -> Some (getRowsRef env i.rows.Value)
                        | _ -> None
                    let outRows = executeOnce rowsInOpt args'
                    acc <- acc @ outRows
                acc
            else
                // bounded parallelism
                let sem = new SemaphoreSlim(dop)
                let tasks =
                    [ for it in items ->
                        task {
                            do! sem.WaitAsync()
                            try
                                let args' = argsForItem st.args it
                                let rowsInOpt =
                                    match st.``in`` with
                                    | Some i when i.rows.IsSome ->
                                        match it with
                                        | :? Types.Row as r -> Some [ r ]
                                        | _ -> Some (getRowsRef env i.rows.Value)
                                    | _ -> None
                                return executeOnce rowsInOpt args'
                            finally
                                sem.Release() |> ignore
                        } ]
                tasks
                |> Task.WhenAll
                |> fun t -> t.Result
                |> Array.toList
                |> List.collect id

    let run (env:ExecEnv) =
        let sp = env.spec

        // 1) Execute steps (HookKey / DotNet / Exe + Parallel/Foreach)
        for node in sp.steps do
            match node with
            | Single st ->
                let rows = runOneStep env st
                env.stepRows.[st.id] <- rows

            | Parallel g ->
                let dop =
                    match g.parallelism with
                    | Some x when x > 1 -> x
                    | Some _ -> 1
                    | None -> 0 // 0 => unbounded
                if dop <= 1 then
                    // sequential within the group
                    for st in g.steps do
                        let rows = runOneStep env st
                        env.stepRows.[st.id] <- rows
                elif dop = 0 then
                    // unbounded concurrency
                    let tasks =
                        g.steps
                        |> List.map (fun st ->
                            task {
                                let rows = runOneStep env st
                                env.stepRows.[st.id] <- rows
                            })
                    Task.WhenAll(tasks) |> fun t -> t.Wait()
                else
                    // bounded concurrency with SemaphoreSlim
                    let sem = new SemaphoreSlim(dop)
                    let tasks =
                        g.steps
                        |> List.map (fun st ->
                            task {
                                do! sem.WaitAsync()
                                try
                                    let rows = runOneStep env st
                                    env.stepRows.[st.id] <- rows
                                finally
                                    sem.Release() |> ignore
                            })
                    Task.WhenAll(tasks) |> fun t -> t.Wait()

        // 2) Validations
        let errs =
            sp.validations
            |> List.choose (fun v ->
                let rows = getRowsRef env v.on
                Validate.evalRule v.id v.rule rows)
        if errs.Length > 0 then
            let msg =
                errs
                |> List.map (fun e -> $"{e.id}: {e.message}")
                |> String.concat "; "
            failwithf "Validation failed: %s" msg

        // 3) Materialize (pre_sql → write → post_sql)
        for m in sp.materialize do
            // resolve connection + table
            let tableName, connStr =
                match parseRef m.``to`` with
                | TableRef name ->
                    match sp.tables |> Map.tryFind name with
                    | Some t ->
                        let ds =
                            sp.datasources
                            |> Map.tryFind t.ds
                            |> Option.defaultWith (fun () -> failwithf "Datasource '%s' not found" t.ds)
                        t.name, ds.conn
                    | None -> failwithf "Unknown table ref '%s'" name
                | Literal s ->
                    let ds =
                        sp.datasources
                        |> Map.tryFind "default"
                        |> Option.defaultWith (fun () -> failwith "No default datasource")
                    s, ds.conn
                | _ -> failwithf "Invalid materialize.to: %s" m.``to``

            m.preSql  |> Option.iter env.sqlExec

            let srcRows = getRowsRef env m.from

            env.bulkEmit
                { conn  = connStr
                  table = tableName
                  mode  = m.mode
                  key   = m.key
                  rows  = srcRows }

            m.postSql |> Option.iter env.sqlExec

// -----------------------------
// Convenience + Scheduling
// -----------------------------
module Runner =
    open Types    open Parse
    open Exec

    type Options =
        { overrideParams : Map<string,string>
          sqlExec        : Exec.SqlExec
          bulkEmit       : Exec.BulkEmit }

    let private applyOverrides (p:Params) (ov:Map<string,string>) =
        let g k d = ov |> Map.tryFind k |> Option.defaultValue d
        { envName  = g "envName"  p.envName
          dbName   = g "dbName"   p.dbName
          connStr  = g "connStr"  p.connStr
          hookNs   = g "hookNs"   p.hookNs
          impl     = g "impl"     p.impl
          batchId  = g "batchId"  p.batchId
          emitMode = g "emitMode" p.emitMode }

    let runFromYaml (yaml:string) (opt:Options) =
        let spec0 = Parse.load yaml
        let spec1 = { spec0 with parameters = applyOverrides spec0.parameters opt.overrideParams }
        let spec  = Parse.interpolateAll spec1

        let env =
            { Exec.spec     = spec
              stepRows      = System.Collections.Concurrent.ConcurrentDictionary<string, Types.Rows>()
              sqlExec       = opt.sqlExec
              bulkEmit      = opt.bulkEmit }

        Exec.run env
        env // return env if caller wants to inspect stepRows

    

///////////////////////////////////////////////////
here are drop-in hook stubs that match your loader/exec logic and the callDotNet reflection (single Run method per type, so no overload ambiguity).

They expose exactly these types for YAML:

module: "POC.Hooks.S0.Fetch.v1.Module", func: "Run"

module: "POC.Hooks.S0.Transform.v1.Module", func: "Run"

POC/Hooks/S0/Fetch/v1.fs
///////////////////////////////////////////////////
namespace POC.Hooks.S0.Fetch.v1

open System
open MMTS.ML.Types

/// S0 Fetch: produces demo rows (Symbol, Price, Ts, BatchId)
/// Signature matches Exec.callDotNet when there is NO input rows:
///   inputs = [| box args |]
type Module =
    static member Run(args: Map<string,string>) : Rows =
        let symbol  = args |> Map.tryFind "item"    |> Option.defaultValue "DEMO"
        let batchId = args |> Map.tryFind "batchId" |> Option.defaultValue (DateTime.UtcNow.ToString("yyyy-MM-dd"))

        // produce a few sample rows
        [ for i in 1 .. 3 ->
            Map [
                "Symbol",   box symbol
                "Price",    box (100.0 + float i)
                "Ts",       box DateTime.UtcNow
                "BatchId",  box batchId
            ] ]

///////////////////////////////////////////////////
POC/Hooks/S0/Transform/v1.fs
///////////////////////////////////////////////////
namespace POC.Hooks.S0.Transform.v1

open System
open MMTS.ML.Types

/// S0 Transform: enriches rows (adds PriceZ and Impl)
/// Signature matches Exec.callDotNet when there ARE input rows:
///   inputs = [| box rows; box args |]
type Module =
    static member Run(rows: Rows, args: Map<string,string>) : Rows =
        let impl = args |> Map.tryFind "impl" |> Option.defaultValue "default"

        rows
        |> List.map (fun r ->
            let price =
                match r |> Map.tryFind "Price" with
                | Some (:? IConvertible as v) -> v.ToDouble(Globalization.CultureInfo.InvariantCulture)
                | Some v -> failwithf "Price not numeric: %A" v
                | None   -> 0.0

            r
            |> Map.add "PriceZ" (box (price / 100.0))
            |> Map.add "Impl"   (box impl)
        )

///////////////////////////////////////////////////
Notes (so it “just works” with your Exec):

Each type has exactly one public static Run, so Type.GetMethod("Run", …) won’t hit an ambiguous overload.

Fetch.Run expects only args, since Exec passes a single arg when rowsInOpt = None.

Transform.Run expects (rows, args), since Exec passes two args when the step has an input (e.g., from s0.fetch).

Column names used here: Symbol, Price, Ts, BatchId, plus transform adds PriceZ, Impl.

If your YAML hooks section points to these fully-qualified types and func: "Run", you’re good.
Want me to also emit a tiny unit test (or a quick console harness) that invokes these hooks through your Exec path to verify end-to-end?
///////////////////////////////////////////////////
awesome—here’s a drop-in Program.fs that:

initializes logging,

reads your YAML (S0_MMTS_ML.yaml) from DOC\,

runs a one-shot “unit test” execution through your MMTS.ML.Runner, and

then starts your scheduler (so heartbeat continues).

It uses safe stub implementations for sqlExec and bulkEmit that log what they’d do.
///////////////////////////////////////////////////
namespace POC
module Program =
    open System
    open System.IO
    open System.Threading
    open System.Threading.Tasks
    open POC.Log
    open POC.Schedule
    open MMTS.ML.Types
    open MMTS.ML.Runner

    // ---------- one-shot YAML test runner ----------
    let private runYamlOnce (yamlPath: string) =
        if not (File.Exists yamlPath) then
            Log.warnf "YAML not found at %s" yamlPath
        else
            try
                let yaml = File.ReadAllText yamlPath

                // override a couple params so today's batch is used and hooks resolve to POC.Hooks
                let overrides =
                    [ "batchId", DateTime.Today.ToString("yyyy-MM-dd")
                      "hookNs" , "POC.Hooks"
                      "impl"   , "v1" ]
                    |> Map.ofList

                // stub side-effects
                let sqlExec : Exec.SqlExec =
                    fun sql -> Log.infof "[sqlExec] %s" sql

                let bulkEmit : Exec.BulkEmit =
                    fun args ->
                        let count = args.rows |> List.length
                        Log.infof "[bulkEmit] mode=%s table=%s rows=%d keys=%A conn=(%s)"
                                  args.mode args.table count args.key args.conn

                let env = runFromYaml yaml { overrideParams = overrides; sqlExec = sqlExec; bulkEmit = bulkEmit }

                // summarize outputs
                env.stepRows
                |> Seq.map (fun kvp -> kvp.Key, kvp.Value.Length)
                |> Seq.sortBy fst
                |> Seq.iter (fun (k, n) -> Log.infof "[result] step=%s rows=%d" k n)

                // (optional) peek first row of each step for sanity
                env.stepRows
                |> Seq.iter (fun kvp ->
                    match kvp.Value with
                    | r::_ ->
                        let preview =
                            r
                            |> Map.toList
                            |> List.truncate 6
                            |> List.map (fun (k,v) -> $"{k}={v}")
                            |> String.concat "; "
                        Log.debugf "[peek] %s -> %s" kvp.Key preview
                    | [] -> ()
                )

            with ex ->
                Log.errorf "YAML run failed: %s" ex.Message
                raise ex

    // ---------- scheduler bootstrap (unchanged behavior) ----------
    let private startScheduler () =
        // sample jobs — adjust to your existing Schedule.ofSync/ofTask usage
        let jobs : ScheduledJob list =
            [ ScheduledJob.ofTask
                  "heartbeat"
                  (Interval (TimeSpan.FromSeconds 10.0))
                  (fun () -> task {
                      Log.infof "Heartbeat at %O" DateTime.Now
                      return ()
                  })
            ]
        Schedule.run jobs

    [<EntryPoint>]
    let main argv =
        // 1) logging
        Log.init("POC", "logs")

        // 2) run the S0 YAML once as a "unit test"
        //    put your S0_MMTS_ML.yaml under DOC\ (or change the path below)
        let yamlPath =
            if argv |> Array.exists ((=) "--yaml") then
                // allow:  POC.exe --yaml "C:\path\to\S0_MMTS_ML.yaml"
                let i = Array.FindIndex(argv, fun a -> a = "--yaml")
                if i >= 0 && i + 1 < argv.Length then argv.[i+1] else @"DOC\S0_MMTS_ML.yaml"
            else
                @"DOC\S0_MMTS_ML.yaml"

        Log.infof "Running YAML test: %s" (Path.GetFullPath yamlPath)
        runYamlOnce yamlPath

        // 3) start scheduler (heartbeat)
        startScheduler ()
        Console.WriteLine("Scheduler started. Press ENTER to exit.")
        Console.ReadLine() |> ignore

        // 4) flush logs
        Log.close()
        0

///////////////////////////////////////////////////
How to use
Put your S0_MMTS_ML.yaml at POC\DOC\S0_MMTS_ML.yaml (or tweak the path).
Build & run:
VS (F5) or dotnet run -c Release from POC\.
Or run exe and optionally point to a different YAML:

.\POC.exe --yaml "C:\MMTS_V2.1\POC\DOC\S0_MMTS_ML.yaml"
--yaml "C:\MMTS_V2.1\YAML\S0_MMTS_ML_2025-09-13.txt"

Watch the console and .\logs\POC-YYYYMMDD.log for:
[sqlExec] lines (for pre/post SQL),
[bulkEmit] summaries (rows/materialize), and
[result] step=<id> rows=<n> summaries per step.
If you want the scheduler’s schedule.cron inside your YAML 
to actually fire the S0 workflow on a cadence, 
I can add a small bridge job that re-reads the YAML and runs it on each tick.
///////////////////////////////////////////////////
here’s a safer Program.fs that wraps YAML parsing/execution in robust try/with, 
logs a clear error, and shows a numbered preview of the YAML 
to help you spot structure issues (like “Expected scalar, got seq”).

Drop-in replacement for your current Program.fs:
///////////////////////////////////////////////////
namespace POC
module Program =
    open System
    open System.IO
    open System.Threading
    open System.Threading.Tasks
    open POC.Log
    open POC.Schedule
    open MMTS.ML.Types
    open MMTS.ML.Runner

    // ---------- helpers ----------
    let private previewFile (path:string) (maxLines:int) =
        try
            File.ReadLines(path)
            |> Seq.truncate maxLines
            |> Seq.mapi (fun i line -> sprintf "%3d| %s" (i+1) line)
            |> String.concat Environment.NewLine
        with _ -> "<unable to read file>"

    // ---------- one-shot YAML test runner with strong error handling ----------
    let private runYamlOnce (yamlPath: string) =
        try
            if not (File.Exists yamlPath) then
                Log.errorf "YAML not found at %s" (Path.GetFullPath yamlPath)
            else
                let yaml =
                    try File.ReadAllText yamlPath
                    with ex ->
                        Log.errorf "Failed to read YAML: %s" ex.Message
                        raise ex

                // Show a small preview to help debugging structure problems
                Log.debugf "YAML preview (first 40 lines):%s%s"
                           Environment.NewLine
                           (previewFile yamlPath 40)

                let env =
                    try
                        // override a couple params so today's batch is used and hooks resolve to POC.Hooks
                        let overrides =
                            [ "batchId", DateTime.Today.ToString("yyyy-MM-dd")
                              "hookNs" , "POC.Hooks"
                              "impl"   , "v1" ]
                            |> Map.ofList

                        // stub side-effects
                        let sqlExec : Exec.SqlExec =
                            fun sql -> Log.infof "[sqlExec] %s" sql

                        let bulkEmit : Exec.BulkEmit =
                            fun args ->
                                let count = args.rows |> List.length
                                Log.infof "[bulkEmit] mode=%s table=%s rows=%d keys=%A conn=(%s)"
                                          args.mode args.table count args.key args.conn

                        // Parse + interpolate — catch YAML/shape errors cleanly
                        let spec0 =
                            try MMTS.ML.Parse.load yaml
                            with ex ->
                                Log.errorf "YAML parse failed: %s" ex.Message
                                // extra hint for common root shape errors
                                Log.warn  "Hint: top-level keys must be scalars (workflow/version/description/params/...)."
                                raise ex

                        let spec  = { spec0 with parameters = 
                                        let p = spec0.parameters
                                        let g (k:string) (d:string) = overrides |> Map.tryFind k |> Option.defaultValue d
                                        { p with batchId = g "batchId" p.batchId
                                                 hookNs  = g "hookNs"  p.hookNs
                                                 impl    = g "impl"    p.impl } }
                                    |> MMTS.ML.Parse.interpolateAll

                        // Execute via runner (this may raise on validation/materialize)
                        let env =
                            try MMTS.ML.Exec.({ spec = spec
                                                stepRows = System.Collections.Concurrent.ConcurrentDictionary<string, Rows>()
                                                sqlExec  = sqlExec
                                                bulkEmit = bulkEmit } |> fun e -> MMTS.ML.Exec.run e; e)
                            with ex ->
                                Log.errorf "Execution failed: %s" ex.Message
                                raise ex

                        env
                    with ex ->
                        // Already logged; rethrow to let caller decide whether to continue
                        raise ex

                // summarize outputs if we got here
                env.stepRows
                |> Seq.map (fun kvp -> kvp.Key, kvp.Value.Length)
                |> Seq.sortBy fst
                |> Seq.iter (fun (k, n) -> Log.infof "[result] step=%s rows=%d" k n)

                env.stepRows
                |> Seq.iter (fun kvp ->
                    match kvp.Value with
                    | r::_ ->
                        let preview =
                            r
                            |> Map.toList
                            |> List.truncate 6
                            |> List.map (fun (k,v) -> $"{k}={v}")
                            |> String.concat "; "
                        Log.debugf "[peek] %s -> %s" kvp.Key preview
                    | [] -> ()
                )

        with ex ->
            // Final catch for anything unexpected
            Log.errorf "YAML run aborted: %s" ex.Message
            // Do not rethrow; allow app to continue (e.g., start scheduler)
            ()

    // ---------- scheduler ----------
    let private startScheduler () =
        let jobs : ScheduledJob list =
            [ ScheduledJob.ofTask
                  "heartbeat"
                  (Interval (TimeSpan.FromSeconds 10.0))
                  (fun () -> task {
                      Log.infof "Heartbeat at %O" DateTime.Now
                      return ()
                  }) ]
        Schedule.run jobs

    [<EntryPoint>]
    let main argv =
        Log.init("POC", "logs")

        // choose YAML path (allow override via --yaml "path")
        let yamlPath =
            let i = Array.FindIndex(argv, fun a -> a = "--yaml")
            if i >= 0 && i + 1 < argv.Length then argv.[i+1] else @"DOC\S0_MMTS_ML.yaml"

        Log.infof "Attempting YAML run: %s" (Path.GetFullPath yamlPath)
        runYamlOnce yamlPath

        // keep the app useful even if YAML failed
        startScheduler ()
        Console.WriteLine("Scheduler started. Press ENTER to exit.")
        Console.ReadLine() |> ignore

        Log.close()
        0

///////////////////////////////////////////////////
fix code...
Good catch—that error comes from the oddball expression:
MMTS.ML.Exec.(
    { ... } |> fun e -> MMTS.ML.Exec.run e; e
)
F# is trying to parse Exec.( ... ) like legacy ML-style type parameters, which triggers the deprecation error. 
The fix is to build the env in a normal let binding, call Exec.run, and then return the env.

Here’s the corrected block you can drop into your runYamlOnce (and it’s clearer too):
///////////////////////////////////////////////////
open System.Collections.Concurrent
open MMTS.ML
open MMTS.ML.Types

// ...

// Build the execution environment, run, then return it
let env =
    let env =
        { Exec.spec     = spec
          Exec.stepRows = ConcurrentDictionary<string, Rows>()
          Exec.sqlExec  = sqlExec
          Exec.bulkEmit = bulkEmit }
    Exec.run env
    env

///////////////////////////////////////////////////
Or, if you prefer to keep full qualifications without the opens:

///////////////////////////////////////////////////
let env =
    let env =
        { MMTS.ML.Exec.spec     = spec
          MMTS.ML.Exec.stepRows = System.Collections.Concurrent.ConcurrentDictionary<string, MMTS.ML.Types.Rows>()
          MMTS.ML.Exec.sqlExec  = sqlExec
          MMTS.ML.Exec.bulkEmit = bulkEmit }
    MMTS.ML.Exec.run env
    env

///////////////////////////////////////////////////
You’re hitting a schema mismatch: your parser expects certain top-level keys to be simple scalars, but your YAML has a mapping under workflow: (with name, kind, etc.). That’s why you get “Expected scalar, got seq/map”.

What your loader expects (from Parse.load)

These lines enforce scalars at the top level:
let workflow    = str root["workflow"]      // must be a string
let version     = str root["version"]       // string
let description = str root["description"]   // string

///////////////////////////////////////////////////
So the YAML must look like this (note: workflow: is a string, not an object):
workflow: "S0_MMTS_ML"
version:  "1.0"
description: "S0 batch workflow"
...
But your file has something like:
...
workflow:
  name: S0_MMTS_ML
  kind: batch
  version: 1.0
...
That structure makes workflow a map, not a scalar — boom
Fix: adjust the YAML to match the parser

Here’s a drop-in YAML skeleton that your current loader 
will accept and that matches the hook stubs I gave you (POC.Hooks.S0.*):

///////////////////////////////////////////////////
workflow: "S0_MMTS_ML"
version:  "1.0"
description: "S0 batch workflow"

params:
  envName:  "dev"
  dbName:   "MMTS_DB"
  connStr:  "Server=.;Database=MMTS_DB;Trusted_Connection=True;Encrypt=False"
  hookNs:   "POC.Hooks"          # Program overrides to this anyway
  impl:     "v1"
  batchId:  "2025-09-13"
  emitMode: "bulk_upsert"

datasources:
  default:
    kind: "mssql"
    conn: "Server=.;Database=MMTS_DB;Trusted_Connection=True;Encrypt=False"

tables:
  s0_out:
    ds:   "default"
    name: "[dbo].[S0_Cadence31]"

hooks:
  s0.fetch:
    module: "POC.Hooks.S0.Fetch.v1.Module"
    func:   "Run"
    kind:   "rows"
  s0.transform:
    module: "POC.Hooks.S0.Transform.v1.Module"
    func:   "Run"
    kind:   "rows"

steps:
  - id: s0.fetch
    uses: "s0.fetch"        # HookKey → resolves via hooks map
    args:
      batchId: "${params.batchId}"
    out:
      rows: "step:s0.fetch.rows"

  - id: s0.transform
    uses:
      module: "POC.Hooks.S0.Transform.v1.Module"  # DotNet form also OK
      func:   "Run"
    in:
      rows: "step:s0.fetch.rows"
    args:
      impl:   "${params.impl}"
    out:
      rows: "step:s0.transform.rows"

validations:
  - id: v1
    on:   "step:s0.fetch.rows"
    rule: "nonempty"
  - id: v2
    on:   "step:s0.transform.rows"
    rule: "schemaMatch(Symbol, Price, Ts, BatchId, PriceZ, Impl)"

materialize:
  - from:  "step:s0.transform.rows"
    to:    "table:s0_out"
    mode:  "${params.emitMode}"
    key:   ["Symbol","BatchId"]
    pre_sql:  "DELETE FROM [dbo].[S0_Cadence31] WHERE BatchId = '${params.batchId}'"
    post_sql: "EXEC [dbo].[sp_AuditLog] '${params.batchId}', '${params.envName}'"

schedule:
  cron: "0 0/5 * * * ?"   # every 5 minutes

///////////////////////////////////////////////////
Prefer to keep your old “rich workflow: block”? (optional code tweak)

If you want to keep:
workflow:
  name: S0_MMTS_ML
  kind: batch
  version: 1.0
...
…you’ll need a tiny change in Parse.load so it accepts either a scalar or a map:
///////////////////////////////////////////////////
let getWorkflow (root:IDictionary<string,YamlNode>) =
    match root.TryGetValue "workflow" with
    | true, (:? YamlScalarNode as s) -> s.Value
    | true, (:? YamlMappingNode as m) ->
        let kv = kvs m
        str kv["name"]   // pull the name as the workflow id
    | _ -> failwith "workflow is required"

let workflow    = getWorkflow root
let version     =
    match root.TryGetValue "version" with
    | true, (:? YamlScalarNode as s) -> s.Value
    | _ ->
        match root.TryGetValue "workflow" with
        | true, (:? YamlMappingNode as m) ->
            let kv = kvs m
            str kv["version"]          // allow version nested under workflow
        | _ -> failwith "version is required"

let description =
    match root.TryGetValue "description" with
    | true, v -> str v
    | _ -> ""   // optional

///////////////////////////////////////////////////
This keeps backward compatibility with both styles.

///////////////////////////////////////////////////
How to pinpoint future parse errors fast

You already log a 40-line preview — good. Add node-type aware errors 
around each str access:
///////////////////////////////////////////////////
let expectScalar (label:string) (n:YamlNode) =
    match n with
    | :? YamlScalarNode as s -> s.Value
    | :? YamlMappingNode -> failwithf "%s: expected scalar, got mapping (object)" label
    | :? YamlSequenceNode -> failwithf "%s: expected scalar, got sequence (array)" label
    | _ -> failwithf "%s: expected scalar" label

// then:
let workflow = expectScalar "top.workflow" root["workflow"]
let version  = expectScalar "top.version"  root["version"]


///////////////////////////////////////////////////
Now the log would say e.g. top.workflow: expected scalar, 
got mapping (object) — instantly telling you the exact key.

///////////////////////////////////////////////////
Great—thanks for pasting it. That YAML matches your parser exactly. 
Nothing in it should trigger “Expected scalar, got seq.” 
So if you’re still seeing that error, it’s almost certainly 
loading a different file (older format) or a different path than you think.

Here’s how to nail it down fast:
1) Confirm the exact file being read

Add this log (you already log a preview, but let’s be explicit):
///////////////////////////////////////////////////
let full = System.IO.Path.GetFullPath yamlPath
Log.infof "Reading YAML from: %s" full
...
Compare that path to the file you just pasted.
...
2) Force a distinctive first line

Put this comment as the first line of your YAML:
# S0_MMTS_ML CURRENT v2025-09-13-TEST
...
When your app logs the first 40 lines, make sure you see that comment.
If you don’t, you’re not reading the file you think you are.
3) Quick sanity loader (isolated)

Drop this tiny check somewhere (or run in F# Interactive) to ensure the loader parses this YAML text:

///////////////////////////////////////////////////
let yamlText = """
# S0_MMTS_ML CURRENT v2025-09-13-TEST
workflow: "S0_MMTS_ML"
version:  "1.0"
description: "S0 batch workflow"
params:
  envName:  "dev"
  dbName:   "MMTS_DB"
  connStr:  "Server=.;Database=MMTS_DB;Trusted_Connection=True;Encrypt=False"
  hookNs:   "POC.Hooks"
  impl:     "v1"
  batchId:  "2025-09-13"
  emitMode: "bulk_upsert"
datasources:
  default: { kind: "mssql", conn: "Server=.;Database=MMTS_DB;Trusted_Connection=True;Encrypt=False" }
tables:
  s0_out:   { ds: "default", name: "[dbo].[S0_Cadence31]" }
hooks:
  s0.fetch:     { module: "POC.Hooks.S0.Fetch.v1.Module", func: "Run", kind: "rows" }
  s0.transform: { module: "POC.Hooks.S0.Transform.v1.Module", func: "Run", kind: "rows" }
steps:
  - id: s0.fetch
    uses: "s0.fetch"
    args: { batchId: "${params.batchId}" }
    out:  { rows: "step:s0.fetch.rows" }
  - id: s0.transform
    uses: { module: "POC.Hooks.S0.Transform.v1.Module", func: "Run" }
    in:   { rows: "step:s0.fetch.rows" }
    args: { impl: "${params.impl}" }
    out:  { rows: "step:s0.transform.rows" }
validations:
  - { id: v1, on: "step:s0.fetch.rows", rule: "nonempty" }
  - { id: v2, on: "step:s0.transform.rows", rule: "schemaMatch(Symbol, Price, Ts, BatchId, PriceZ, Impl)" }
materialize:
  - from:  "step:s0.transform.rows"
    to:    "table:s0_out"
    mode:  "${params.emitMode}"
    key:   ["Symbol","BatchId"]
    pre_sql:  "DELETE FROM [dbo].[S0_Cadence31] WHERE BatchId = '${params.batchId}'"
    post_sql: "EXEC [dbo].[sp_AuditLog] '${params.batchId}', '${params.envName}'"
schedule: { cron: "0 0/5 * * * ?" }
"""

let spec0 = MMTS.ML.Parse.load yamlText   // should not throw
let spec  = MMTS.ML.Parse.interpolateAll spec0
printfn "OK: workflow=%s, steps=%d" spec.workflow spec.steps.Length

///////////////////////////////////////////////////
If that prints OK, the parser is fine; the problem is just which file your program is reading.

4) Make error messages pinpoint the key

If you still want extra clarity, update your Y.str helper 
to show which field failed and what node type it saw:
///////////////////////////////////////////////////
module private Y =
    let private kind (n:YamlNode) =
        match n with
        | :? YamlScalarNode -> "scalar"
        | :? YamlMappingNode -> "mapping"
        | :? YamlSequenceNode -> "sequence"
        | _ -> n.GetType().Name

    let strL (label:string) (n:YamlNode) =
        match n with
        | :? YamlScalarNode as s -> s.Value |> string
        | _ -> failwithf "%s: expected scalar, got %s" label (kind n)

    // then use strL in Parse.load for top-level keys:
    let workflow    = strL "top.workflow"    root["workflow"]
    let version     = strL "top.version"     root["version"]
    let description = strL "top.description" root["description"]

///////////////////////////////////////////////////
Now errors read like top.workflow: expected scalar, got mapping, which is immediately actionable.

5) Common gotchas to check

Multiple files named similarly (e.g., S0_MMTS_ML.yaml vs S0_MMTS_ML.yml) in different folders.

Running from a working directory that changes the relative path 
(@"DOC\S0_MMTS_ML.yaml" may point somewhere else).

VS “Copy to Output Directory” mismatch (if you’re reading from bin\...). 
Prefer reading from the project folder using AppContext.BaseDirectory or Directory.GetCurrentDirectory() and logging the resolved absolute path.

If you want, show me the log line that prints the full path 
and the first 5 preview lines your app is reading; I’ll tell you immediately 
whether it’s the right file or not.
///////////////////////////////////////////////////
can you give code to pretty-print from all roots to nodes?
yes — if you want to debug your YAML parse deeply, the trick is to walk the YamlNode tree and pretty-print all roots down to leaves with indentation. That way you’ll see exactly where the parser sees a mapping instead of a scalar.

Here’s a drop-in F# helper:
///////////////////////////////////////////////////
open System
open YamlDotNet.RepresentationModel

module YamlDebug =

    let rec private dumpNode (indent:int) (node:YamlNode) =
        let pad = String(' ', indent)
        match node with
        | :? YamlScalarNode as s ->
            printfn "%s- SCALAR: %s" pad (if isNull s.Value then "<null>" else s.Value)
        | :? YamlMappingNode as m ->
            printfn "%s- MAPPING {" pad
            for kv in m.Children do
                let k = kv.Key :?> YamlScalarNode
                printfn "%s  Key: %s" pad k.Value
                dumpNode (indent+4) kv.Value
            printfn "%s}" pad
        | :? YamlSequenceNode as seq ->
            printfn "%s- SEQUENCE [" pad
            seq.Children
            |> Seq.iteri (fun i child ->
                printfn "%s  Item %d:" pad i
                dumpNode (indent+4) child )
            printfn "%s]" pad
        | other ->
            printfn "%s- UNKNOWN node type: %s" pad (other.GetType().FullName)

    /// Pretty-print entire YAML document tree
    let dumpYaml (yamlText:string) =
        let ys = YamlStream()
        ys.Load(new System.IO.StringReader(yamlText))
        let doc = ys.Documents.[0]
        printfn "ROOT DOCUMENT"
        dumpNode 0 doc.RootNode

///////////////////////////////////////////////////
ok, all good, got the roots/nodes pretty-prints...thank you...now, 
let wire scheduler to run the hook for fetch and transform...can you give code?
///////////////////////////////////////////////////
awesome — let’s make the scheduler actually run your S0 workflow (Fetch → Transform) on a cron, 
by reading your YAML each tick.
Below is a focused, drop-in wiring you can paste into Program.fs. It:
reads DOC\S0_MMTS_ML.yaml
extracts schedule.cron from YAML (falls back to an interval if missing)
defines a non-overlapping scheduled job that runs the YAML (with logging + safe error handling)
keeps your heartbeat
///////////////////////////////////////////////////
namespace POC
module Program =
    open System
    open System.IO
    open System.Threading
    open System.Threading.Tasks
    open POC.Log
    open POC.Schedule
    open MMTS.ML.Types

    // -----------------------------
    // Stubs for side-effects used by Exec
    // -----------------------------
    let private sqlExec : MMTS.ML.Exec.SqlExec =
        fun sql -> Log.infof "[sqlExec] %s" sql

    let private bulkEmit : MMTS.ML.Exec.BulkEmit =
        fun args ->
            let count = args.rows |> List.length
            Log.infof "[bulkEmit] mode=%s table=%s rows=%d keys=%A conn=(%s)"
                      args.mode args.table count args.key args.conn

    // -----------------------------
    // One-shot runner (reads YAML and executes it once)
    // -----------------------------
    let private runS0Once (yamlPath: string) : Task =
        task {
            try
                let full = Path.GetFullPath yamlPath
                if not (File.Exists full) then
                    Log.errorf "YAML not found: %s" full
                else
                    let yaml = File.ReadAllText full
                    // Parse + interpolate (apply a few convenient overrides)
                    let spec0   = MMTS.ML.Parse.load yaml
                    let p0      = spec0.parameters
                    let p'      =
                        { p0 with
                            batchId = DateTime.Today.ToString("yyyy-MM-dd")
                            hookNs  = if String.IsNullOrWhiteSpace p0.hookNs then "POC.Hooks" else p0.hookNs
                            impl    = if String.IsNullOrWhiteSpace p0.impl   then "v1"        else p0.impl }
                    let spec    = { spec0 with parameters = p' } |> MMTS.ML.Parse.interpolateAll

                    // Build exec env, run, log summaries
                    let env =
                        { MMTS.ML.Exec.spec     = spec
                          MMTS.ML.Exec.stepRows = System.Collections.Concurrent.ConcurrentDictionary<string, Rows>()
                          MMTS.ML.Exec.sqlExec  = sqlExec
                          MMTS.ML.Exec.bulkEmit = bulkEmit }

                    MMTS.ML.Exec.run env

                    // Summaries
                    env.stepRows
                    |> Seq.map (fun kv -> kv.Key, kv.Value.Length)
                    |> Seq.sortBy fst
                    |> Seq.iter (fun (k, n) -> Log.infof "[result] step=%s rows=%d" k n)

            with ex ->
                Log.errorf "S0 run failed: %s" ex.Message
        }

    // -----------------------------
    // Create a ScheduledJob for S0 based on YAML's schedule.cron
    // (falls back to every 5 minutes if missing)
    // -----------------------------
    let private makeS0Job (yamlPath: string) : ScheduledJob =
        // read cron from YAML if present
        let cronExpr =
            try
                let yaml = File.ReadAllText yamlPath
                let spec = MMTS.ML.Parse.load yaml
                match spec.schedule with
                | Some s when not (String.IsNullOrWhiteSpace s.cron) -> s.cron
                | _ -> "0 0/5 * * * ?" // default: every 5 minutes
            with _ ->
                "0 0/5 * * * ?"

        // guard against overlapping executions
        let running = ref 0

        let action () = task {
            if Interlocked.Exchange(running, 1) = 1 then
                Log.warn "S0 job skipped; previous run still in progress."
            else
                try
                    Log.infof "S0 scheduled run starting (cron=%s)" cronExpr
                    do! runS0Once yamlPath
                    Log.info "S0 scheduled run completed."
                finally
                    running := 0
        }

        { Id = "s0_workflow"
          Schedule = Cron cronExpr
          Action = action }

    // -----------------------------
    // Heartbeat job (unchanged)
    // -----------------------------
    let private heartbeatJob : ScheduledJob =
        ScheduledJob.ofTask
            "heartbeat"
            (Interval (TimeSpan.FromSeconds 10.0))
            (fun () -> task {
                Log.infof "Heartbeat at %O" DateTime.Now
                return ()
            })

    // -----------------------------
    // Main
    // -----------------------------
    [<EntryPoint>]
    let main argv =
        Log.init("POC", "logs")

        // resolve YAML path (allow override with --yaml "path")
        let yamlPath =
            let i = Array.FindIndex(argv, fun a -> a = "--yaml")
            if i >= 0 && i + 1 < argv.Length then argv.[i+1]
            else @"DOC\S0_MMTS_ML.yaml"
        let fullYaml = Path.GetFullPath yamlPath
        Log.infof "Using YAML: %s" fullYaml

        // (optional) run once at startup
        runS0Once fullYaml
        |> Async.AwaitTask
        |> Async.RunSynchronously

        // start scheduler with S0 job + heartbeat
        let jobs = [ makeS0Job fullYaml; heartbeatJob ]
        Schedule.run jobs

        Console.WriteLine("Scheduler started. Press ENTER to exit.")
        Console.ReadLine() |> ignore

        Log.close()
        0

///////////////////////////////////////////////////
What this does

runS0Once: calls your loader/parser, interpolates params, executes the pipeline, 
and logs row counts for each step (s0.fetch, s0.transform).

makeS0Job: reads schedule.cron from the YAML (e.g., 0 0/5 * * * ?) and builds a Quartz job that:

prevents overlaps using an atomic flag,

logs start/finish, and

runs runS0Once.

Main:

logs which YAML it’s using,

runs once at startup (so you can see outputs immediately),

then schedules periodic runs + your heartbeat.

If you want misfire handling (e.g., “fire immediately if we missed a tick”), 
we can change the TriggerBuilder to apply WithMisfireHandlingInstructionFireAndProceed() 
inside your Schedule module. Happy to add that too.
///////////////////////////////////////////////////
got error....The type 'Task' does not match the type 'Task<unit>' on...Action = action...see below ....
let action () = task { 
    if Interlocked.Exchange(running, 1) = 1 then
        Log.warn "S0 job skipped; previous run still in progress."
    else
        try
            Log.infof "S0 scheduled run starting (cron=%s)" cronExpr
            do! runS0Once yamlPath
            Log.info "S0 scheduled run completed."
        finally
            running := 0
}

{ 
    Id = "s0_workflow"
    Schedule = Cron cronExpr
    Action = action
}
///////////////////////////////////////////////////
You’re right—your ScheduledJob.Action is typed as unit -> Task<unit>, 
but task { ... } without an explicit return yields Task (non-generic). 
Make the action return unit explicitly on all paths.

Here’s the fixed snippet:
///////////////////////////////////////////////////
open System.Threading

let private makeS0Job (yamlPath: string) : ScheduledJob =
    let cronExpr =
        try
            let yaml = System.IO.File.ReadAllText yamlPath
            let spec = MMTS.ML.Parse.load yaml
            match spec.schedule with
            | Some s when not (System.String.IsNullOrWhiteSpace s.cron) -> s.cron
            | _ -> "0 0/5 * * * ?"
        with _ -> "0 0/5 * * * ?"

    let running = ref 0

    // NOTE: return () on every path so the CE yields Task<unit>
    let action () = task {
        if Interlocked.Exchange(running, 1) = 1 then
            Log.warn "S0 job skipped; previous run still in progress."
            return ()   // <-- ensures Task<unit> in the fast-skip branch
        else
            try
                Log.infof "S0 scheduled run starting (cron=%s)" cronExpr
                do! runS0Once yamlPath
                Log.info "S0 scheduled run completed."
                return () // <-- explicit unit return for the main path
            finally
                running := 0
    }

    { Id = "s0_workflow"
      Schedule = Cron cronExpr
      Action = action }   // now has type unit -> Task<unit>

///////////////////////////////////////////////////
If you prefer to keep actions as unit -> Task instead, 
you can also change your ScheduledJob definition to:
///////////////////////////////////////////////////
type ScheduledJob =
  { Id       : string
    Schedule : ScheduleKind
    Action   : unit -> System.Threading.Tasks.Task } // non-generic Task

///////////////////////////////////////////////////
…but given your current signature expects Task<unit>, 
the first fix (explicit return ()) is the cleanest.
///////////////////////////////////////////////////
but subtle detail here:

task { ... } in F# always produces a Task<'T> where 'T is inferred from your last return/return!.

If you don’t return () (unit), the compiler may infer Task (non-generic), 
but then you’ve got an implicit mismatch with the rest of your pipeline 
(your ScheduledJob.Action is typed unit -> Task, or unit -> Task<unit>, 
depending on how you defined it).
///////////////////////////////////////////////////
let action () : Task = task {
    if Interlocked.Exchange(running, 1) = 1 then
        Log.warn "S0 job skipped; previous run still in progress."
    else
        try
            Log.infof "S0 scheduled run starting (cron=%s)" cronExpr
            do! runS0Once yamlPath
            Log.info "S0 scheduled run completed."
        finally
            running := 0
}

///////////////////////////////////////////////////
Here the CE yields Task<unit>, which is upcast to Task because of the return type annotation.
///////////////////////////////////////////////////
let action () : Task<unit> = task {
    if Interlocked.Exchange(running, 1) = 1 then
        Log.warn "S0 job skipped; previous run still in progress."
        return ()   // ensures Task<unit>
    else
        try
            Log.infof "S0 scheduled run starting (cron=%s)" cronExpr
            do! runS0Once yamlPath
            Log.info "S0 scheduled run completed."
            return () 
        finally
            running := 0
}

///////////////////////////////////////////////////
So it depends on how you typed Action in ScheduledJob.

If it’s unit -> Task → your current fix is perfectly fine.

If it’s unit -> Task<unit> → add explicit return ().

Do you want me to recheck your latest Schedule.fs 
definition of ScheduledJob so we’re 100% aligned?
///////////////////////////////////////////////////
here’s a tiny, drop-in pretty-printer for the Hooks registry 
that you can call right after parsing/interpolation. 
It prints neat columns (key, module, func, kind) 
and checks whether the target type/method actually exists in loaded assemblies.

1) Add this helper (e.g., Program.fs or a new Debug.fs)
///////////////////////////////////////////////////
module HooksDebug =
    open System
    open System.Reflection
    open MMTS.ML.Types
    open POC.Log

    // resolve a type by full name across loaded assemblies
    let private tryResolveType (fullName: string) =
        match Type.GetType(fullName, throwOnError = false) with
        | null ->
            AppDomain.CurrentDomain.GetAssemblies()
            |> Seq.tryPick (fun asm ->
                let t = asm.GetType(fullName, throwOnError = false)
                if isNull t then None else Some t)
        | t -> Some t

    /// Pretty-print the hooks table and validate type/method presence
    let dumpHooks (spec: Spec) =
        if spec.hooks.IsEmpty then
            Log.warn "hooks: <empty>"
        else
            // build rows
            let rows =
                spec.hooks
                |> Map.toList
                |> List.map (fun (key, h) ->
                    // status: TYPE? NOFUNC? OK
                    let status =
                        try
                            match tryResolveType h.moduleName with
                            | None      -> "TYPE?"
                            | Some typ  ->
                                let mi = typ.GetMethod(h.func, BindingFlags.Public ||| BindingFlags.Static)
                                if isNull mi then "NOFUNC" else "OK"
                        with _ -> "ERROR"
                    (key, h.moduleName, h.func, h.kind, status))

            // column widths
            let wKey   = max 3 (rows |> List.maxBy (fun (k,_,_,_,_) -> k.Length) |> fun (k,_,_,_,_) -> k.Length)
            let wMod   = max 6 (rows |> List.maxBy (fun (_,m,_,_,_) -> m.Length) |> fun (_,m,_,_,_) -> m.Length)
            let wFunc  = max 4 (rows |> List.maxBy (fun (_,_,f,_,_) -> f.Length) |> fun (_,_,f,_,_) -> f.Length)
            let wKind  = max 4 (rows |> List.maxBy (fun (_,_,_,k,_) -> k.Length) |> fun (_,_,_,k,_) -> k.Length)
            let wStat  = 6

            let header =
                $"| {"key".PadRight wKey} | {"module".PadRight wMod} | {"func".PadRight wFunc} | {"kind".PadRight wKind} | {"status".PadRight wStat} |"

            let sep =
                "|-" + String('-', wKey) + "-|-" + String('-', wMod) + "-|-" + String('-', wFunc) +
                "-|-" + String('-', wKind) + "-|-" + String('-', wStat) + "-|"

            Log.info "hooks registry:"
            Log.info header
            Log.info sep
            rows
            |> List.iter (fun (k,m,f,kind,status) ->
                Log.info (sprintf "| %s | %s | %s | %s | %s |"
                              (k.PadRight wKey)
                              (m.PadRight wMod)
                              (f.PadRight wFunc)
                              (kind.PadRight wKind)
                              (status.PadRight wStat)))

///////////////////////////////////////////////////
2) Call it right after you parse + interpolate

Where you currently have:
let spec0 = MMTS.ML.Parse.load yaml
let spec  = { spec0 with parameters = p' } |> MMTS.ML.Parse.interpolateAll
HooksDebug.dumpHooks spec

///////////////////////////////////////////////////
F#’s interpolated string syntax is stricter than C#’s.
You can’t directly nest { "string".PadRight n } inside the $"" — the parser gets confused.

Two easy fixes:
Fix 1: pre-bind each column name
///////////////////////////////////////////////////
let keyHdr   = "key".PadRight wKey
let modHdr   = "module".PadRight wMod
let funcHdr  = "func".PadRight wFunc
let kindHdr  = "kind".PadRight wKind
let statHdr  = "status".PadRight wStat

let header =
    $"| {keyHdr} | {modHdr} | {funcHdr} | {kindHdr} | {statHdr} |"

///////////////////////////////////////////////////
Fix 2: don’t interpolate string literals, just concat
...
let header =
    "| " + "key".PadRight wKey +
    " | " + "module".PadRight wMod +
    " | " + "func".PadRight wFunc +
    " | " + "kind".PadRight wKind +
    " | " + "status".PadRight wStat + " |"

///////////////////////////////////////////////////
glad the hooks registry dump looks sharp.
let’s now pretty-print the step results (env.stepRows) 
after execution so you can see what fetch/transform produced.
1) add a helper module
///////////////////////////////////////////////////
module ResultsDebug =
    open System
    open MMTS.ML.Types
    open POC.Log

    /// Pretty-print a set of step results (env.stepRows)
    let dumpResults (stepRows: System.Collections.Concurrent.ConcurrentDictionary<string, Rows>) =
        if stepRows.IsEmpty then
            Log.warn "results: <empty>"
        else
            Log.info "results summary:"
            for kv in stepRows |> Seq.sortBy (fun kv -> kv.Key) do
                let key  = kv.Key
                let rows = kv.Value
                Log.infof "step=%s rows=%d" key rows.Length

                if not rows.IsEmpty then
                    // take a sample row
                    let sample = rows |> List.head
                    let cols   = sample |> Map.toList |> List.map fst

                    // header
                    let header = String.Join(" | ", cols)
                    Log.infof "  cols: %s" header

                    // first few rows
                    rows
                    |> Seq.truncate 5
                    |> Seq.iter (fun r ->
                        let vals =
                            cols
                            |> List.map (fun c ->
                                match r.TryFind c with
                                | Some v -> string v
                                | None -> "")
                        Log.infof "  data: %s" (String.Join(" | ", vals)) )

                    if rows.Length > 5 then
                        Log.infof "  ... (%d more rows)" (rows.Length - 5)

///////////////////////////////////////////////////
2) call it after execution

in your runS0Once (after Exec.run env), add:
ResultsDebug.dumpResults env.stepRows
///////////////////////////////////////////////////
here’s a drop-in POC.Hooks.S0.Fetch.v1.Module that:

ensures the DB objects exist (creates dbo.S0_Cadence31 table if missing, installs TVP + MERGE proc + index),

fetches OHLC bars via your existing reader using your Example column maps, and

returns them as Rows (list of maps) for the MMTS pipeline to consume.

I’ve used your uploaded modules so it fits your environment (reader/robust/writer/sql-setup)
POC/Hooks/S0/Fetch/v1.fs
///////////////////////////////////////////////////
namespace POC.Hooks.S0.Fetch.v1

open System
open System.Data
open System.Text
open Microsoft.Data.SqlClient

open MMTS.ML.Types                  // Rows = list< Map<string,obj> >
open MMTS.S0                        // DataFeed, DataField, ColumnMap, DbConfig, etc.
open MMTS.S0.reader                 // readOhlc
open MMTS.S0.S0Writer               // (not used here, but handy if you later want to upsert raw)
open MMTS.S0.S0_SqlSetup            // .sql text for TVP/proc/index install (your file)
open MMTS.S0.Robust                 // writeCadence31 (not used here)
open MMTS.S0                        // Examples.dbCfg lives in your uploaded Program.txt

module private DbInit =

    /// Split a T-SQL script on GO batch separators (line with only 'GO', case-insensitive).
    let private splitOnGo (sql:string) =
        let lines = sql.Replace("\r\n","\n").Split('\n')
        let batches = System.Collections.Generic.List<string>()
        let sb = StringBuilder()
        let flush () =
            if sb.Length > 0 then
                batches.Add(sb.ToString().TrimEnd())
                sb.Clear() |> ignore
        for line in lines do
            if line.Trim().Equals("GO", StringComparison.OrdinalIgnoreCase) then
                flush()
            else
                sb.AppendLine(line) |> ignore
        flush()
        batches |> Seq.toList |> List.filter (fun b -> b.Trim() <> "")

    /// Execute a SQL statement (no results).
    let private execNonQuery (connStr:string) (sql:string) =
        use conn = new SqlConnection(connStr)
        use cmd  = new SqlCommand(sql, conn)
        cmd.CommandType <- CommandType.Text
        conn.Open()
        cmd.ExecuteNonQuery() |> ignore

    /// Create the S0_Cadence31 table if it doesn't exist (columns match your TVP & proc).
    let ensureCadence31Table (connStr:string) =
        let ddl = """
IF OBJECT_ID(N'dbo.S0_Cadence31', N'U') IS NULL
BEGIN
  CREATE TABLE dbo.S0_Cadence31
  (
    DataFeed   VARCHAR(32)  NOT NULL,
    Symbol     VARCHAR(32)  NOT NULL,
    StatName   VARCHAR(64)  NOT NULL,
    DataField  VARCHAR(64)  NULL,
    TradeDate  DATE         NOT NULL,
    Dt         DATETIME2(0) NOT NULL,
    Raw        FLOAT(53)    NULL,
    Med        FLOAT(53)    NULL,
    Mad        FLOAT(53)    NULL,
    Scale      FLOAT(53)    NULL,
    MZ         FLOAT(53)    NULL,
    Weight     FLOAT(53)    NULL,
    Clean      FLOAT(53)    NULL,
    Cadence    FLOAT(53)    NULL,
    Source     VARCHAR(64)  NULL
  );
END
"""
        execNonQuery connStr ddl

    /// Install / refresh the TVP type + MERGE proc + index using your script (handles GO).
    let ensureTvpAndProc (connStr:string) =
        let batches = splitOnGo S0_SqlSetup.sql
        for b in batches do
            execNonQuery connStr b

    /// One-shot idempotent install (safe to call every run)
    let ensureAll (connStr:string) =
        ensureCadence31Table connStr
        ensureTvpAndProc connStr


module private ParseArg =
    let get (k:string) (d:string) (m:Map<string,string>) =
        m |> Map.tryFind k |> Option.defaultValue d

    let getDate (k:string) (d:DateTime) (m:Map<string,string>) =
        match m |> Map.tryFind k with
        | None -> d
        | Some s ->
            match DateTime.TryParse s with
            | true, dt -> dt
            | _ -> d

    let tryFeed (s:string) =
        match s.Trim().ToUpperInvariant() with
        | "EOD1DAY_YAHOO" -> Some DataFeed.EOD1Day_YAHOO
        | "EOD1DAY_IBRK"  -> Some DataFeed.EOD1Day_IBRK
        | "EOD5SECS"      -> Some DataFeed.EOD5Secs
        | _ -> None

    /// Accept comma-separated list of feeds; default to EOD1Day_YAHOO
    let feeds (m:Map<string,string>) =
        match m |> Map.tryFind "feeds" with
        | None -> [ DataFeed.EOD1Day_YAHOO ]
        | Some csv ->
            csv.Split([|','|], StringSplitOptions.RemoveEmptyEntries)
            |> Array.choose (fun s -> tryFeed s)
            |> Array.toList
            |> function
               | [] -> [ DataFeed.EOD1Day_YAHOO ]
               | xs -> xs

open ParseArg
open DbInit

type Module =
    /// Run signature (no input rows): returns Rows for the next step.
    /// Args expected (defaults in parens):
    /// - connStr: SQL connection string (required in real runs)
    /// - symbol : ("AAPL")
    /// - t0     : from date ("2024-01-01")
    /// - t1     : to date   ("2024-12-31")
    /// - feeds  : CSV list of feeds ("EOD1Day_YAHOO,EOD1Day_IBRK")
    static member Run(args: Map<string,string>) : Rows =
        // 1) Connection string (try args, then a dev default)
        let connStr =
            match args |> Map.tryFind "connStr" with
            | Some s when not (String.IsNullOrWhiteSpace s) -> s
            | _ -> "Server=.;Database=0_1Day_V01;Trusted_Connection=True;Encrypt=False"

        // 2) Ensure DB objects (table, TVP type, MERGE proc, index)
        ensureAll connStr  // uses your S0_SqlSetup.sql to install TVP/proc/index

        // 3) Pull request parameters
        let symbol = get "symbol" "AAPL" args
        let t0     = getDate "t0" (DateTime(2024,1,1)) args
        let t1     = getDate "t1" (DateTime(2024,12,31)) args
        let feedsL = feeds args

        // 4) Use your Example mappings for EOD1Day/EOD5Secs
        //    (from your Program.txt: Examples.dbCfg)
        let dbCfg =
            // MMTS.S0.Examples.dbCfg was included with your upload; reference it here.
            // If you later move it, inject via params instead.
            MMTS.S0.Examples.dbCfg

        // 5) Read bars for each feed, accumulate into Rows
        let mutable out : Rows = []

        for feed in feedsL do
            let bars = reader.readOhlc connStr dbCfg feed symbol t0 t1
            // Map each OHLC row into a generic Row (Map<string,obj>) for MMTS
            let feedName = feed.ToString()
            let rows =
                bars
                |> List.map (fun b ->
                    Map [
                        "DataFeed",  box feedName
                        "Symbol",    box symbol
                        "T",         box b.T
                        "O",         box b.O
                        "H",         box b.H
                        "L",         box b.L
                        "C",         box b.C
                        "V",         box b.V
                        "Count",     box b.Count
                        "Wap",       box b.Wap
                    ])
            out <- out @ rows

        // 6) Return combined Rows to the pipeline (e.g., for S0.Transform)
        out

///////////////////////////////////////////////////
How to call it from YAML

You already have this in your YAML (good):
///////////////////////////////////////////////////
hooks:
  s0.fetch:
    module: "POC.Hooks.S0.Fetch.v1.Module"
    func:   "Run"
    kind:   "rows"

steps:
  - id: s0.fetch
    uses: "s0.fetch"
    args:
      connStr: "${params.connStr}"
      symbol:  "AAPL"
      t0:      "2024-01-01"
      t1:      "2024-12-31"
      feeds:   "EOD1Day_YAHOO,EOD1Day_IBRK"
    out:
      rows: "step:s0.fetch.rows"

///////////////////////////////////////////////////
Short answer: schemaMatch checks the columns in the in-memory Rows coming from a step, 
not the DB table. So your rule must match the shape your s0.transform actually emits.

Right now your rule expects:
schemaMatch(Symbol, Price, Ts, BatchId, PriceZ, Impl)
…but your S0 path (Cadence-31) uses the CadRow/TVP shape that looks like:
DataFeed, Symbol, StatName, DataField, TradeDate, Dt, Raw, Med, Mad, Scale, MZ, Weight, Clean, Cadence, Source

(that’s how the writer/TVP/proc and records are defined).

What to do

If s0.transform still outputs your earlier “analytics” row (Price, PriceZ, …), keep your current rule.

If s0.transform now produces Cadence-31 rows to materialize into dbo.S0_Cadence31, change the rule to match that schema, e.g.:
....
validations:
  - id: v1
    on:   "step:s0.fetch.rows"
    rule: "nonempty"

  # validate the exact shape you're about to materialize
  - id: v2
    on:   "step:s0.transform.rows"
    rule: "schemaMatch(DataFeed, Symbol, StatName, DataField, TradeDate, Dt, Raw, Med, Mad, Scale, MZ, Weight, Clean, Cadence, Source)"

S0 run failed: Validation failed: v2: schema mismatch; need ...
[DataFeed,Symbol,StatName,DataField,TradeDate,Dt,Raw,Med,Mad,Scale,MZ,Weight,Clean,Cadence,Source]

///////////////////////////////////////////////////
  - id: v3
    on: "step:s0.transform.rows"
    rule: "unique(Dt)"
(Or a stronger uniqueness over a concatenated key you emit, e.g., Symbol+Dt if your transform outputs that.)
///////////////////////////////////////////////////
1) Log the exact columns your step produced (diff vs expected)

Drop this helper somewhere (e.g., ResultsDebug.fs) and 
call it in your catch block when the validation fails:
///////////////////////////////////////////////////
module ResultsDebug =
    open System
    open MMTS.ML.Types
    open POC.Log

    let private colsOf (rows: Rows) =
        match rows with
        | [] -> Set.empty
        | r::_ -> r |> Map.toList |> List.map fst |> Set.ofList

    let dumpSchemaDiff (stepId: string) (rows: Rows) (expected: string list) =
        let exp = Set.ofList expected
        let got = colsOf rows
        Log.infof "schema for step=%s" stepId
        Log.infof "  expected: %s" (String.Join(", ", expected))
        Log.infof "  got     : %s" (String.Join(", ", got |> Set.toList))
        let missing = exp - got
        let extra   = got - exp
        if not (Set.isEmpty missing) then Log.errorf "  MISSING: %s" (String.Join(", ", missing))
        if not (Set.isEmpty extra)   then Log.warnf  "  EXTRA  : %s" (String.Join(", ", extra))

///////////////////////////////////////////////////
Then, in your runS0Once catch:
....
with ex ->
    if ex.Message.StartsWith("Validation failed") then
        // try to dump the transform step’s schema
        let expected = [
          "DataFeed"; "Symbol"; "StatName"; "DataField"; "TradeDate"; "Dt";
          "Raw"; "Med"; "Mad"; "Scale"; "MZ"; "Weight"; "Clean"; "Cadence"; "Source"
        ]
        match env.stepRows.TryGetValue "s0.transform" with
        | true, r -> ResultsDebug.dumpSchemaDiff "s0.transform" r expected
        | _ -> Log.error "no rows stored for step 's0.transform'"
    Log.errorf "S0 run failed: %s" ex.Message
///////////////////////////////////////////////////
This will print “MISSING/EXTRA” keys so you know exactly what to rename or add.
2) Make sure your transform really emits the Cadence-31 columns

Your Cadence-31 record and writer are defined with those exact names (case-sensitive): DataFeed, Symbol, StatName, DataField, TradeDate, Dt, Raw, Med, Mad, Scale, MZ, Weight, Clean, Cadence, Source.

Also your SQL TVP/proc expects the same names.

If your transform builds CadRow list, convert each to a Row like this:
///////////////////////////////////////////////////
let private cadRowToRow (r: MMTS.S0.Robust.CadRow) : MMTS.ML.Types.Row =
    Map [
      "DataFeed",  box r.DataFeed
      "Symbol",    box r.Symbol
      "StatName",  box r.StatName
      "DataField", box r.DataField
      "TradeDate", box r.TradeDate
      "Dt",        box r.Dt
      "Raw",       box r.Raw
      "Med",       box r.Med
      "Mad",       box r.Mad
      "Scale",     box r.Scale
      "MZ",        box r.MZ
      "Weight",    box r.Weight
      "Clean",     box r.Clean
      "Cadence",   box r.Cadence
      "Source",    box r.Source
    ]

///////////////////////////////////////////////////
…and return a Rows:

let rows : Rows = cadRows |> List.map cadRowToRow
rows


///////////////////////////////////////////////////
3) If you really want to validate against your intermediate analytics shape

If your current s0.transform is still emitting analytics columns 
like Price, Ts, PriceZ, Impl, change your YAML back to that schema:
...- id: v2
  on:   "step:s0.transform.rows"
  rule: "schemaMatch(Symbol, Price, Ts, BatchId, PriceZ, Impl)"

  ...
  …but if you’re moving towards Cadence-31 and S0_Cadence31 upsert, 
  keep the Cadence schema and make the transform return those columns.
///////////////////////////////////////////////////
4) Sanity: how your upstream data flows

reader.readOhlc selects T,O,H,L,C,V,Count,Wap and returns Ohlc records; names are exactly those (note T and Count, Wap).

You convert to change-triples and/or to CadRow (Cadence-31) with fields listed above.

The TVP/proc use the same column names as CadRow.

So the correct validation for what you’re about to materialize into S0_Cadence31 is the Cadence-31 schema. 
Once your transform returns those keys, schemaMatch will pass.
///////////////////////////////////////////////////
Absolutely—here’s a ready-to-drop-in POC.Hooks.S0.Transform.v1 that takes the OHLC rows from your s0.fetch, 
computes Cadence-31 for both the raw OHLC series and the chg_* series, 
and returns the canonical S0_Cadence31 row shape your validation/materialize expect.

It leans directly on your uploaded modules for OHLC reading/shape, change-series, and Cadence-31 logic.
///////////////////////////////////////////////////
namespace POC.Hooks.S0.Transform.v1

open System
open MMTS.ML.Types

open MMTS.S0
open MMTS.S0.OhlcChg           // buildChangeTriples, toOhlcChgWithCountAndWap
open MMTS.S0.RobustCadence31   // cadence31 (OHLC), plus types for Cadence31-chg helpers
open MMTS.S0.Cadence31ChgWriter
open MMTS.S0.reader            // just for types; we don't hit the DB here

[<AutoOpen>]
module private Util =
    let tryAs<'T> (k:string) (m:Row) =
        match Map.tryFind k m with
        | Some v ->
            try Some (v :?> 'T) with _ -> None
        | None -> None

    let getStr k (m:Row) = tryAs<string> k m |> Option.defaultValue ""
    let getF   k (m:Row) =
        match Map.tryFind k m with
        | Some (:? float as f) -> f
        | Some v ->
            match v with
            | :? int    as i -> float i
            | :? int64  as i -> float i
            | :? double as d -> d
            | :? single as s -> float s
            | :? decimal as d -> float d
            | _ -> Double.NaN
        | None -> Double.NaN

    let getDt k (m:Row) =
        match Map.tryFind k m with
        | Some (:? DateTime as t) -> t
        | Some v ->
            match v with
            | :? string as s ->
                let ok, dt = DateTime.TryParse s
                if ok then dt else DateTime.MinValue
            | _ -> DateTime.MinValue
        | None -> DateTime.MinValue

    /// Convert our robust CadRow record into pipeline Row (Map<string,obj>)
    let cadRowToRow (r: MMTS.S0.Robust.CadRow) : Row =
        Map [
          "DataFeed",  box r.DataFeed
          "Symbol",    box r.Symbol
          "StatName",  box r.StatName
          "DataField", box r.DataField
          "TradeDate", box r.TradeDate
          "Dt",        box r.Dt
          "Raw",       box r.Raw
          "Med",       box r.Med
          "Mad",       box r.Mad
          "Scale",     box r.Scale
          "MZ",        box r.MZ
          "Weight",    box r.Weight
          "Clean",     box r.Clean
          "Cadence",   box r.Cadence
          "Source",    box r.Source
        ]

    /// Build (Dt,value) series for a specific OHLC field
    let buildSeries (fieldName:string) (rows: Row list) : (DateTime * float) list =
        rows
        |> List.map (fun r ->
            let ts = getDt   "T" r
            let v  =
                match fieldName with
                | "OP"   -> getF "O" r
                | "HI"   -> getF "H" r
                | "LO"   -> getF "L" r
                | "CL"   -> getF "C" r
                | "VO"   -> getF "V" r
                | "WAP"  ->
                    let w = getF "Wap" r
                    if Double.IsNaN w then
                        // fallback: simple average if wap missing
                        let o = getF "O" r
                        let h = getF "H" r
                        let l = getF "L" r
                        let c = getF "C" r
                        (o + h + l + c) / 4.0
                    else w
                | "COUNT" -> getF "Count" r
                | _       -> Double.NaN
            ts, v)
        |> List.sortBy fst

    /// Derive Ohlc_chg list (with optional Count/Wap support) from input Rows
    let toOhlcChgList (rows: Row list) : OhlcChg.Ohlc_chg list =
        rows
        |> OhlcChg.toOhlcChgWithCountAndWap
               (fun r -> getDt "T" r)
               (fun r -> getF  "O" r)
               (fun r -> getF  "H" r)
               (fun r -> getF  "L" r)
               (fun r -> getF  "C" r)
               (fun r -> getF  "V" r)
               (fun r ->          // Count (optional)
                    let x = getF "Count" r
                    if Double.IsNaN x then None else Some x)
               (fun r ->          // Wap (optional)
                    let x = getF "Wap" r
                    if Double.IsNaN x then None else Some x)

    /// Project a group (feed,symbol) + one named series into CadRow list using cadence31
    let seriesToCadRows
        (feed:string)
        (symbol:string)
        (statName:string)
        (dataField:string)
        (source:string)
        (series: (DateTime*float) list)
        : MMTS.S0.Robust.CadRow list =
        RobustCadence31.cadence31 series
        |> List.map (fun (dt, raw, med, mad, scale, mz, w, clean, cad) ->
            { DataFeed  = feed
              Symbol    = symbol
              StatName  = statName
              DataField = dataField
              TradeDate = dt.Date
              Dt        = dt
              Raw       = raw
              Med       = med
              Mad       = mad
              Scale     = scale
              MZ        = mz
              Weight    = w
              Clean     = clean
              Cadence   = cad
              Source    = source })

type Module =
    /// Transform: takes OHLC rows from fetch and emits Cadence-31 rows (OHLC + chg_*).
    /// Args:
    ///   - mode: "both" (default) | "ohlc" | "chg"
    ///   - keep: optional CSV list of chg_* names to include (e.g., "chg_WAP,chg_CL")
    ///   - dataFieldOHLC: override DataField for OHLC rows (default "OHLC")
    ///   - dataFieldCHG : override DataField for chg rows  (default "OHLC_CHG")
    static member Run(rows: Rows, args: Map<string,string>) : Rows =
        // group by (DataFeed, Symbol) so mixed feeds/symbols work
        let groups =
            rows
            |> List.groupBy (fun r -> getStr "DataFeed" r, getStr "Symbol" r)

        let mode =
            args |> Map.tryFind "mode"
                 |> Option.defaultValue "both"
                 |> fun s -> s.Trim().ToLowerInvariant()

        let keepNamesOpt =
            args |> Map.tryFind "keep"
                 |> Option.map (fun csv ->
                        csv.Split([|','|], StringSplitOptions.RemoveEmptyEntries)
                        |> Array.map (fun s -> s.Trim())
                        |> Array.toList)

        let dfOHLC = args |> Map.tryFind "dataFieldOHLC" |> Option.defaultValue "OHLC"
        let dfCHG  = args |> Map.tryFind "dataFieldCHG"  |> Option.defaultValue "OHLC_CHG"

        let mutable outCadRows : MMTS.S0.Robust.CadRow list = []

        for ((feed,symbol), rowsOne) in groups do
            let sourceTag = if String.IsNullOrWhiteSpace feed then "UNKNOWN" else feed

            // ---- A) OHLC Cadence-31 ----
            if mode = "both" || mode = "ohlc" then
                // build series for OP/HI/LO/CL/VO/WAP (+COUNT if present)
                let series =
                    [ "OP"; "HI"; "LO"; "CL"; "VO"; "WAP" ]
                    |> List.map (fun name -> name, buildSeries name rowsOne)
                    |> List.filter (fun (_, s) -> not s.IsEmpty)

                // include COUNT only if present (any non-NaN)
                let hasCount =
                    rowsOne |> List.exists (fun r -> not (Double.IsNaN (getF "Count" r)))
                let series =
                    if hasCount then series @ [ "COUNT", buildSeries "COUNT" rowsOne ]
                    else series

                for (statName, s) in series do
                    let cad = Util.seriesToCadRows feed symbol statName dfOHLC sourceTag s
                    outCadRows <- outCadRows @ cad

            // ---- B) chg_* Cadence-31 ----
            if mode = "both" || mode = "chg" then
                let ohlcChg = toOhlcChgList rowsOne
                let triples = OhlcChg.buildChangeTriples ohlcChg
                let cadChgRows =
                    Cadence31ChgWriter.cadence31_chg_toCadRows
                        triples feed symbol dfCHG sourceTag keepNamesOpt
                outCadRows <- outCadRows @ cadChgRows

        // Convert to pipeline Rows and return
        outCadRows |> List.map cadRowToRow

    /// Overload: if the engine accidentally calls transform without input rows, be graceful.
    static member Run(args: Map<string,string>) : Rows =
        // No input rows ⇒ nothing to transform.
        []

///////////////////////////////////////////////////
Notes

Input shape expected from fetch: each row carries DataFeed, Symbol, T, O, H, L, C, V, Count?, Wap?. That’s exactly what your fetch returns; we read those keys, build series, and compute Cadence-31.

OHLC cadence uses your RobustCadence31.cadence31 with canonical output mapped into CadRow (DataFeed, Symbol, StatName, DataField="OHLC", TradeDate, Dt, Raw, Med, Mad, Scale, MZ, Weight, Clean, Cadence, Source).

chg cadence builds chg_* triples via OhlcChg.buildChangeTriples and turns them into CadRow via Cadence31ChgWriter.cadence31_chg_toCadRows. You can filter with keep: "chg_WAP,chg_CL,...".

Output schema exactly matches your TVP/proc & your schemaMatch rule for s0.transform.rows.
///////////////////////////////////////////////////
YAML reminder (your current config already fits)
...
- id: s0.transform
  uses:
    module: "POC.Hooks.S0.Transform.v1.Module"
    func:   "Run"
  in:
    rows: "step:s0.fetch.rows"
  args:
    mode: "both"                    # both | ohlc | chg
    keep: "chg_WAP,chg_CL"          # optional filter for chg_*
    dataFieldOHLC: "OHLC"
    dataFieldCHG:  "OHLC_CHG"
  out:
    rows: "step:s0.transform.rows"

validations:
  - id: v2
    on:   "step:s0.transform.rows"
    rule: "schemaMatch(DataFeed, Symbol, StatName, DataField, TradeDate, Dt, Raw, Med, Mad, Scale, MZ, Weight, Clean, Cadence, Source)"


///////////////////////////////////////////////////
YAML reminder (your current config already fits)
...
- id: s0.transform
  uses:
    module: "POC.Hooks.S0.Transform.v1.Module"
    func:   "Run"
  in:
    rows: "step:s0.fetch.rows"
  args:
    mode: "both"                    # both | ohlc | chg
    keep: "chg_WAP,chg_CL"          # optional filter for chg_*
    dataFieldOHLC: "OHLC"
    dataFieldCHG:  "OHLC_CHG"
  out:
    rows: "step:s0.transform.rows"

validations:
  - id: v2
    on:   "step:s0.transform.rows"
    rule: "schemaMatch(DataFeed, Symbol, StatName, DataField, TradeDate, Dt, Raw, Med, Mad, Scale, MZ, Weight, Clean, Cadence, Source)"

///////////////////////////////////////////////////
I’ll wire a POC.Hooks.S0.Transform.v1.Module implementation so your YAML can actually run the s0.transform step and produce Cadence-31 shaped rows that will pass your schemaMatch.

We already have:

reader.readOhlc to fetch OHLC bars

OhlcChg.buildChangeTriples etc.

Robust.CadRow and Robust.writeCadence31

So the transform hook’s job is to:

Accept the fetch rows (from s0.fetch).

Convert them into CadRow list (with all required columns).

Return them as Rows (Map<string,obj> list).

Here’s a scaffold:
///////////////////////////////////////////////////
namespace POC.Hooks.S0.Transform.v1

open System
open MMTS.ML.Types
open MMTS.S0
open MMTS.S0.OhlcChg
open MMTS.S0.reader
open MMTS.S0.Robust

module Module =

    /// Convert CadRow -> Row (Map<string,obj>) for schemaMatch & materialize
    let private cadRowToRow (r: CadRow) : Row =
        Map [
            "DataFeed",  box r.DataFeed
            "Symbol",    box r.Symbol
            "StatName",  box r.StatName
            "DataField", box r.DataField
            "TradeDate", box r.TradeDate
            "Dt",        box r.Dt
            "Raw",       box r.Raw
            "Med",       box r.Med
            "Mad",       box r.Mad
            "Scale",     box r.Scale
            "MZ",        box r.MZ
            "Weight",    box r.Weight
            "Clean",     box r.Clean
            "Cadence",   box r.Cadence
            "Source",    box r.Source
        ]

    /// Example transform: run cadence-31 over CL (closing price) series
    let Run (_rows: Rows) (args: Map<string,string>) : Rows =
        let connStr = args.["connStr"]
        let symbol  = args.["symbol"]
        let fromDt  = DateTime.Parse(args.["from"])
        let toDt    = DateTime.Parse(args.["to"])
        let feed    = DataFeed.EOD1Day_IBRK   // hardcoded for now, parametrize later
        let cfg     = Program.Examples.dbCfg

        // read bars
        let bars = reader.readOhlc connStr cfg feed symbol fromDt toDt
        if bars.IsEmpty then [] else

        // build close series
        let closeSeries = bars |> List.map (fun b -> b.T, b.C) |> List.sortBy fst

        // cadence-31
        let enriched = RobustCadence31.cadence31 closeSeries

        // map to CadRow
        let cadRows =
            enriched
            |> List.map (fun (dt, raw, med, mad, scale, mz, w, clean, cad) ->
                { DataFeed = feed.ToString()
                  Symbol   = symbol
                  StatName = "CL"
                  DataField= "OHLC"
                  TradeDate= dt.Date
                  Dt       = dt
                  Raw      = raw
                  Med      = med
                  Mad      = mad
                  Scale    = scale
                  MZ       = mz
                  Weight   = w
                  Clean    = clean
                  Cadence  = cad
                  Source   = feed.ToString() })

        // convert to Rows
        cadRows |> List.map cadRowToRow

///////////////////////////////////////////////////
How it fits:

The YAML passes ${params.connStr}, ${params.batchId}, etc.

You can extend the args in YAML (symbol, from, to) to control the transform.

The Rows it returns now have exactly the dbo.S0_Cadence31 schema (so schemaMatch passes).

Your materialize rule will then upsert into S0_Cadence31.
///////////////////////////////////////////////////
Question: do you want this transform to always write via 
Robust.writeCadence31 inside the hook (side-effect), or only to return rows for Exec → materialize?

Right now I gave you the second style (return rows only) so the REPL pipeline controls DB I/O.
///////////////////////////////////////////////////
short answer: yes—the YAML-driven MMTS (hooks + schedule + logging) is a big step up in flexibility, repeatability, and team velocity. here’s a crisp compare to your old, code-wired approach.

what you gain

Decoupling & composability
Fetch/transform/materialize are swappable “hooks”. Pipelines are just data (YAML). New flows = editing YAML, not shipping new binaries.

Reproducibility
A pipeline run is fully described by {YAML + params + code version}. You can re-run any batchId exactly.

Environment portability
One spec, many envs: ${params.connStr}, ${params.envName}, ${params.batchId} make promotion (dev→test→prod) easy.

Operability

Scheduler: run via cron or interval without changing code.

Non-overlap guard: no double-firing.

Logging: consistent Serilog logs (console + rolling files), debug dumps (hooks table, result schemas).

Observability & Validation
validations: (e.g., nonempty, schemaMatch, unique) fail fast before writes; much safer than “hope and merge”.

REPL & dry-runs
You can run a YAML once (ad-hoc), inspect rows, then schedule it. Great for exploratory work and incident response.

Governance & reviews
YAML is tiny, diff-able, and code-review friendly. Hook code remains small and unit-testable.

tradeoffs (and how to handle them)

YAML is untyped → schema drift & typos

Mitigation: add a JSON-schema or custom parser checks (you already did expectScalar, pretty-print tree, dumpHooks, dumpResults, dumpSchemaDiff).

Add CI: validate YAML on PR; run a “dry” parse + schemaMatch against golden samples.

Parser/engine complexity

Keep Parse small and boring, push logic into hooks.

Version the spec (you already store version: "1.0"). If you evolve syntax, keep 1.x backward-compatible or provide a migrator.

Debugging data shape

You already have ResultsDebug.dumpResults and dumpSchemaDiff. Keep them on by default in dev.

Performance under concurrency

You’ve added foreach + group parallelism. Keep bounded DOP where sources are DB-bound; for CPU-heavy transforms, allow higher DOP.

Materialize with TVP + MERGE is already idempotent and efficient—good.

when it shines

Multiple symbols/feeds, many small variants of the same flow.

Rapid iteration on transforms without destabilizing the worker.

On-call operations: tweak cron / parameters in YAML, re-run a batch with a different batchId quickly.

Audits: show the exact YAML + code SHA that produced a table state.

when to keep code-wired paths

Ultra-tight, latency-critical loops where YAML parsing overhead, dynamic reflection, or extra checks are too costly (rare for S0 batch).

Highly bespoke jobs with unusual control flow (then write a specialized hook and call it from a minimal YAML).

recommended guardrails (quick wins)

YAML schema check in CI (fail on unknown keys, missing required blocks).

Strict key names in transform output; validate with schemaMatch on the step you materialize.

Version & pin hooks: e.g., POC.Hooks.S0.Transform.v1.Module so you can ship v2 alongside during transitions.

Secrets: never store credentials in YAML; use ${params.connStr} from env/KeyVault.

Idempotent writes: you’re already using TVP + MERGE + keys—keep it.

Runbook: “How to dry-run a YAML, read logs, and re-run with a different batchId”.

bottom line

For S0 (and most batch workflows), the YAML-first + small hooks architecture gives you:

faster changes (edit YAML, not code),

safer runs (validations + idempotent materialize),

easier ops (cron + logging),

better reproducibility.

It’s a clear upgrade over the old monolithic, code-wired implementation. If you want, I can add a tiny mmts validate <yaml> CLI that runs parse + interpolation + hook resolution + a schema dry-run, so every edit is “green” before it hits the scheduler.

ChatGPT can make
///////////////////////////////////////////////////

